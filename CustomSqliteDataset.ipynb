{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the final endition of a SQLITE dataset with 'efficient' DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r\"C:\\Users\\jv97\\Desktop\\github\\Neutrino-Machine-Learning\\raw_data\"\n",
    "filename = \"rasmus_classification_muon_3neutrino_3mio.db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from pandas import read_sql\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "class custom_db_dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, filepath, filename, features, targets, TrTV, event_nos = None, x_transform = None, y_transform = None):\n",
    "        self.filepath = filepath\n",
    "        self.filename = filename\n",
    "        self.features = features #Should be string of features, eg: \"charge_log10, time, pulse_width, SRTInIcePulses, dom_x, dom_y, dom_z\"\n",
    "        self.targets = targets #Should be string of targets, eg: \"azimuth, zenith, energy_log10\"\n",
    "        self.TrTV = TrTV #Should be cumulative sum of percentages for \"Tr(ain)T(est)V(alidation)\"\" sets.\n",
    "        \n",
    "        self.con = sqlite3.connect('file:'+os.path.join(self.filepath,self.filename+'?mode=ro'),uri=True)\n",
    "        \n",
    "        if isinstance(event_nos,type(None)):\n",
    "            self.event_nos = np.asarray(read_sql(\"SELECT event_no FROM truth\",self.con)).reshape(-1)\n",
    "        else:\n",
    "            self.event_nos = event_nos\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"length method, number of events\"\"\"\n",
    "        return len(self.event_nos)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, int):\n",
    "            return self.get_single(index)\n",
    "        if isinstance(index, list):\n",
    "            return self.get_list(index)\n",
    "    \n",
    "    def get_single(self,index):\n",
    "        query = f\"SELECT {self.features} FROM features WHERE event_no = {self.event_nos[index]}\"\n",
    "        x = torch.tensor(read_sql(query,self.con).values)\n",
    "\n",
    "        query = f\"SELECT {self.targets} FROM truth WHERE event_no = {self.event_nos[index]}\"\n",
    "        y = torch.tensor(read_sql(query,self.con).values)\n",
    "        return Data(x=x, y=y)\n",
    "    \n",
    "    def get_list(self,index):\n",
    "        query = f\"SELECT {self.features} FROM features WHERE event_no IN {tuple(self.event_nos[index])} {self.where_extra_x}\"\n",
    "        x = torch.tensor(read_sql(query,self.con).values)\n",
    "\n",
    "        query = f\"SELECT {self.targets} FROM truth WHERE event_no IN {tuple(self.event_nos[index])} {self.where_extra_y}\"\n",
    "        y = torch.tensor(read_sql(query,self.con).values)\n",
    "        return Data(x=x, y=y)\n",
    "    \n",
    "    def return_self(self,event_nos):\n",
    "        return custom_db_dataset(self.filepath,\n",
    "                                 self.filename,\n",
    "                                 self.features,\n",
    "                                 self.targets,\n",
    "                                 self.TrTV,\n",
    "                                 event_nos,\n",
    "                                 self.x_transform,\n",
    "                                 self.y_transform)\n",
    "    \n",
    "    def train(self):\n",
    "        return self.return_self(self.event_nos[:int(TrTV[0]*self.__len__())])\n",
    "\n",
    "    def test(self):\n",
    "        return self.return_self(self.event_nos[int(TrTV[0]*self.__len__()):int(TrTV[1]*self.__len__())])\n",
    "\n",
    "    def val(self):\n",
    "        return self.return_self(self.event_nos[int(TrTV[1]*self.__len__()):int(TrTV[2]*self.__len__())])\n",
    "    \n",
    "    def return_dataloaders(self, batch_size):\n",
    "        from torch.utils.data import BatchSampler, DataLoader, SequentialSampler, RandomSampler\n",
    "        def collate(batch):\n",
    "            return Batch.from_data_list(batch)\n",
    "\n",
    "        train_loader = DataLoader(dataset = self.train(),\n",
    "                                  collate_fn = collate,\n",
    "                                  sampler = BatchSampler(RandomSampler(self.train()),\n",
    "                                                         batch_size=batch_size,\n",
    "                                                         drop_last=False))\n",
    "        \n",
    "        test_loader = DataLoader(dataset = a.test(),\n",
    "                                 collate_fn = collate,\n",
    "                                 sampler = SequentialSampler(RandomSampler(self.test()),\n",
    "                                                             batch_size=batch_size,\n",
    "                                                             drop_last=False))\n",
    "        \n",
    "        val_loader = DataLoader(dataset = self.val(),\n",
    "                                collate_fn = collate,\n",
    "                                sampler = BatchSampler(RandomSampler(self.val()),\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       drop_last=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.97000000e+02, 1.20102893e+08, 6.00000000e+00, ...,\n",
       "        8.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "       [1.98000000e+02, 1.20102893e+08, 5.00000000e+01, ...,\n",
       "        8.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "       [1.99000000e+02, 1.20102893e+08, 5.80000000e+01, ...,\n",
       "        8.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "       ...,\n",
       "       [2.47000000e+02, 1.20102893e+08, 4.50000000e+01, ...,\n",
       "        8.00000000e+00, 1.00000000e+00, 1.00000000e+00],\n",
       "       [2.48000000e+02, 1.20102893e+08, 4.70000000e+01, ...,\n",
       "        8.00000000e+00, 1.00000000e+00, 0.00000000e+00],\n",
       "       [2.49000000e+02, 1.20102893e+08, 8.60000000e+01, ...,\n",
       "        8.00000000e+00, 1.00000000e+00, 1.00000000e+00]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_sql(f\"SELECT * FROM features WHERE event_no = {a.event_nos[5]}\",a.con)\n",
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Below here is experimentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_db_dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, filepath, filename):\n",
    "        self.con = sqlite3.connect('file:'+os.path.join(filepath,filename+'?mode=ro'),uri=True)\n",
    "#         self.event_nos, self.event_lengths = np.unique(read_sql(\"SELECT event_no FROM features\",self.con).event_no.values,return_counts=True)\n",
    "#         self.event_nos = np.asarray(read_sql(\"SELECT event_no FROM truth\",self.con)).reshape(-1)\n",
    "        df = read_sql(f\"SELECT COUNT(*), event_no FROM features GROUP BY event_no\",self.con)\n",
    "        print(\"Memory usage: \",df.memory_usage())\n",
    "        self.event_lengths = np.asarray(df.iloc[:,0])\n",
    "        self.cumsum = np.append(0,self.event_lengths.cumsum())\n",
    "        self.event_nos = np.asarray(df.iloc[:,1])\n",
    "        del df\n",
    "        self.features = \"charge_log10, time, pulse_width, SRTInIcePulses, dom_x, dom_y, dom_z\"\n",
    "        self.targets = \"energy_log10\"\n",
    "#         self.where_extra_x = \" and SRTInIcePulses = 1\"\n",
    "#         self.where_extra_y = \"\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"length method, number of events\"\"\"\n",
    "        return len(self.event_nos)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, int):\n",
    "            return self.get_single(index)\n",
    "#         if isinstance(index, list):\n",
    "#             return self.get_list(index)\n",
    "    \n",
    "#     def get_single(self,index):\n",
    "#         query = f\"SELECT {self.features} FROM features  {self.where_extra_x}\"\n",
    "#         x = torch.tensor(read_sql(query,self.con).to_numpy())\n",
    "\n",
    "#         query = f\"SELECT {self.targets} FROM truth WHERE event_no = {self.event_nos[index]} {self.where_extra_y}\"\n",
    "#         y = torch.tensor(read_sql(query,self.con).to_numpy())\n",
    "#         return Data(x=x, y=y)\n",
    "    \n",
    "    def get_single(self,index):\n",
    "        query = f\"SELECT {self.features} FROM features LIMIT {self.cumsum[index]},{self.event_lengths[index]}\"\n",
    "        x = torch.tensor(read_sql(query, self.con).to_numpy())\n",
    "        \n",
    "        query = f\"SELECT {self.targets} FROM truth WHERE event_no = {self.event_nos[index]}\"\n",
    "        y = torch.tensor(read_sql(query,self.con).to_numpy())\n",
    "        return Data(x=x,y=y)\n",
    "        \n",
    "    \n",
    "#     def get_list(self,index):\n",
    "#         query = f\"SELECT event_no, {self.features} FROM features WHERE event_no in {tuple(self.event_nos[index])} {self.where_extra_x}\"\n",
    "#         events = read_sql(query,self.con)\n",
    "#         x = torch.tensor(events.iloc[:,1:].to_numpy())\n",
    "\n",
    "#         query = f\"SELECT {self.targets} FROM truth WHERE event_no in {tuple(self.event_nos[index])} {self.where_extra_y}\"\n",
    "#         y = torch.tensor(read_sql(query,self.con).to_numpy())\n",
    "        \n",
    "#         data_list = []\n",
    "#         _, events = np.unique(events.event_no.values.flatten(), return_counts = True)\n",
    "#         for tmp_x, tmp_y in zip(torch.split(x, events.tolist()), y):\n",
    "#             data_list.append(Data(x=tmp_x,y=tmp_y))\n",
    "#         return data_list\n",
    "    \n",
    "#     def query(self, query_string):\n",
    "#         \"\"\"run a query and return the result\"\"\"\n",
    "#         self.cursor.execute(query_string)\n",
    "#         return self.cursor.fetchall()\n",
    "    \n",
    "#     def process_query(self, items):\n",
    "#         return read_sql(items)\n",
    "\n",
    "class custom_db_dataset1(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, filepath, filename, event_nos = None):\n",
    "        self.filepath = filepath\n",
    "        self.filename = filename\n",
    "        self.con = sqlite3.connect('file:'+os.path.join(self.filepath,self.filename+'?mode=ro'),uri=True)\n",
    "        if isinstance(event_nos,type(None)):\n",
    "            self.event_nos = np.asarray(read_sql(\"SELECT event_no FROM truth LIMIT 50\",self.con)).reshape(-1)\n",
    "        else:\n",
    "            self.event_nos = event_nos\n",
    "        self.features = \"charge_log10, time, pulse_width, SRTInIcePulses, dom_x, dom_y, dom_z\"\n",
    "        self.targets = \"event_no, energy_log10\"\n",
    "        self.where_extra_x = \"\"#\" and SRTInIcePulses = 1\"\n",
    "        self.where_extra_y = \"\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"length method, number of events\"\"\"\n",
    "        return len(self.event_nos)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, int):\n",
    "            return self.get_single(index)\n",
    "        if isinstance(index, list):\n",
    "            return self.get_list(index)\n",
    "    \n",
    "    def get_single(self,index):\n",
    "        query = f\"SELECT {self.features} FROM features WHERE event_no = {self.event_nos[index]} {self.where_extra_x}\"\n",
    "        x = torch.tensor(read_sql(query,self.con).to_numpy())\n",
    "\n",
    "        query = f\"SELECT {self.targets} FROM truth WHERE event_no = {self.event_nos[index]} {self.where_extra_y}\"\n",
    "        y = torch.tensor(read_sql(query,self.con).to_numpy())\n",
    "        return Data(x=x, y=y)\n",
    "    \n",
    "    def get_list(self,index):\n",
    "        query = f\"SELECT {self.features} FROM features WHERE event_no IN {tuple(self.event_nos[index])} {self.where_extra_x}\"\n",
    "        x = torch.tensor(read_sql(query,self.con).to_numpy())\n",
    "\n",
    "        query = f\"SELECT {self.targets} FROM truth WHERE event_no IN {tuple(self.event_nos[index])} {self.where_extra_y}\"\n",
    "        y = torch.tensor(read_sql(query,self.con).to_numpy())\n",
    "        return Data(x=x, y=y)\n",
    "    \n",
    "    def train(self):\n",
    "        return custom_db_dataset1(self.filepath,self.filename,event_nos = self.event_nos[:25])\n",
    "    \n",
    "    def val(self):\n",
    "        return custom_db_dataset1(self.filepath,self.filename,event_nos = self.event_nos[25:])\n",
    "\n",
    "class custom_db_dataset2(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, filepath, filename):\n",
    "        self.con = sqlite3.connect('file:'+os.path.join(filepath,filename+'?mode=ro'),uri=True)\n",
    "        self.event_nos = np.asarray(read_sql(\"SELECT event_no FROM truth\",self.con)).reshape(-1)\n",
    "        \n",
    "        if not all(self.event_nos == np.sort(self.event_nos)):\n",
    "            print('ERROR: indexing for this database is not yet supported!!')\n",
    "        \n",
    "        self.features = \"charge_log10, time, pulse_width, SRTInIcePulses, dom_x, dom_y, dom_z\"\n",
    "        self.targets = \"energy_log10\"\n",
    "        self.where_extra_x = \" and SRTInIcePulses = 1\"\n",
    "        self.where_extra_y = \"\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"length method, number of events\"\"\"\n",
    "        return len(self.event_nos)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, int):\n",
    "            return self.get_single(index)\n",
    "#         if isinstance(index, list):\n",
    "#             return self.get_list(index)\n",
    "    \n",
    "    def get_single(self,index):\n",
    "        query = f\"SELECT {self.features} FROM features WHERE event_no BETWEEN {self.event_nos[index-1]} AND {self.event_nos[index+1]}\"\n",
    "        x = torch.tensor(read_sql(query,self.con).to_numpy())\n",
    "\n",
    "        query = f\"SELECT {self.targets} FROM truth WHERE event_no = {self.event_nos[index]}\"\n",
    "        y = torch.tensor(read_sql(query,self.con).to_numpy())\n",
    "        return Data(x=x, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = custom_db_dataset(filepath,filename)\n",
    "a = custom_db_dataset1(filepath,filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "    return Batch.from_data_list(batch)\n",
    "\n",
    "from torch.utils.data import BatchSampler, DataLoader, SequentialSampler, RandomSampler\n",
    "\n",
    "b = DataLoader(dataset = a.train(),\n",
    "               sampler = BatchSampler(RandomSampler(a.train()),\n",
    "                                      batch_size=5,\n",
    "                                      drop_last=False),\n",
    "               collate_fn = collate)\n",
    "b2 = DataLoader(dataset = a.val(),\n",
    "               sampler = BatchSampler(RandomSampler(a.val()),\n",
    "                                      batch_size=5,\n",
    "                                      drop_last=False),\n",
    "               collate_fn = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008000612258911133\n",
      "tensor([15., 22., 24., 25., 29.], dtype=torch.float64)\n",
      "0.0049991607666015625\n",
      "tensor([ 4., 17., 19., 21., 23.], dtype=torch.float64)\n",
      "0.003998756408691406\n",
      "tensor([ 1.,  5.,  9., 13., 16.], dtype=torch.float64)\n",
      "0.0050013065338134766\n",
      "tensor([ 3.,  6.,  8., 18., 20.], dtype=torch.float64)\n",
      "0.003998517990112305\n",
      "tensor([ 0.,  2., 10., 14., 27.], dtype=torch.float64)\n",
      "0.005000114440917969\n",
      "tensor([32., 37., 44., 47., 55.], dtype=torch.float64)\n",
      "0.003002166748046875\n",
      "tensor([30., 31., 49., 50., 51.], dtype=torch.float64)\n",
      "0.004001617431640625\n",
      "tensor([35., 45., 46., 48., 57.], dtype=torch.float64)\n",
      "0.0030014514923095703\n",
      "tensor([38., 39., 40., 53., 58.], dtype=torch.float64)\n",
      "0.003999948501586914\n",
      "tensor([34., 42., 54., 56., 59.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "i = 0\n",
    "for batch in b:\n",
    "    print(time.time() - start_time)\n",
    "    print(batch.y[:,0]- a.event_nos.min())\n",
    "    start_time = time.time()\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "i = 0\n",
    "for batch in b2:\n",
    "    print(time.time() - start_time)\n",
    "    print(batch.y[:,0]- a.event_nos.min())\n",
    "    start_time = time.time()\n",
    "    i += 1\n",
    "    if i > 100:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2828373908996582\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "data_list = []\n",
    "for i in range(512):\n",
    "    data_list.append(a[i])\n",
    "b = collate(data_list)\n",
    "print(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(batch=[21091], x=[21091, 7], y=[512, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiEpochsDataLoader(torch.utils.data.DataLoader):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._DataLoader__initialized = False\n",
    "        self.batch_sampler = _RepeatSampler(self.batch_sampler)\n",
    "        self._DataLoader__initialized = True\n",
    "        self.iterator = super().__iter__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.batch_sampler.sampler)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self)):\n",
    "            yield next(self.iterator)\n",
    "\n",
    "class _RepeatSampler(object):\n",
    "    \"\"\" Sampler that repeats forever.\n",
    "    Args:\n",
    "        sampler (Sampler)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, sampler):\n",
    "        self.sampler = sampler\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield from iter(self.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = MultiEpochsDataLoader(a, batch_size=512,shuffle=False,collate_fn = collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
