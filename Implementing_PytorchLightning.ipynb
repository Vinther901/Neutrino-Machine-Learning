{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Trainer in module pytorch_lightning.trainer.trainer:\n",
      "\n",
      "class Trainer(pytorch_lightning.trainer.properties.TrainerProperties, pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin, pytorch_lightning.trainer.model_hooks.TrainerModelHooksMixin, pytorch_lightning.trainer.optimizers.TrainerOptimizersMixin, pytorch_lightning.trainer.logging.TrainerLoggingMixin, pytorch_lightning.trainer.training_tricks.TrainerTrainingTricksMixin, pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin, pytorch_lightning.trainer.deprecated_api.DeprecatedDistDeviceAttributes, pytorch_lightning.trainer.deprecated_api.DeprecatedTrainerAttributes)\n",
      " |  Trainer(logger: Union[pytorch_lightning.loggers.base.LightningLoggerBase, Iterable[pytorch_lightning.loggers.base.LightningLoggerBase], bool] = True, checkpoint_callback: bool = True, callbacks: Union[List[pytorch_lightning.callbacks.base.Callback], pytorch_lightning.callbacks.base.Callback, NoneType] = None, default_root_dir: Union[str, NoneType] = None, gradient_clip_val: float = 0, process_position: int = 0, num_nodes: int = 1, num_processes: int = 1, gpus: Union[int, str, List[int], NoneType] = None, auto_select_gpus: bool = False, tpu_cores: Union[int, str, List[int], NoneType] = None, log_gpu_memory: Union[str, NoneType] = None, progress_bar_refresh_rate: Union[int, NoneType] = None, overfit_batches: Union[int, float] = 0.0, track_grad_norm: Union[int, float, str] = -1, check_val_every_n_epoch: int = 1, fast_dev_run: Union[int, bool] = False, accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1, max_epochs: Union[int, NoneType] = None, min_epochs: Union[int, NoneType] = None, max_steps: Union[int, NoneType] = None, min_steps: Union[int, NoneType] = None, limit_train_batches: Union[int, float] = 1.0, limit_val_batches: Union[int, float] = 1.0, limit_test_batches: Union[int, float] = 1.0, limit_predict_batches: Union[int, float] = 1.0, val_check_interval: Union[int, float] = 1.0, flush_logs_every_n_steps: int = 100, log_every_n_steps: int = 50, accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator, NoneType] = None, sync_batchnorm: bool = False, precision: int = 32, weights_summary: Union[str, NoneType] = 'top', weights_save_path: Union[str, NoneType] = None, num_sanity_val_steps: int = 2, truncated_bptt_steps: Union[int, NoneType] = None, resume_from_checkpoint: Union[pathlib.Path, str, NoneType] = None, profiler: Union[pytorch_lightning.profiler.profilers.BaseProfiler, bool, str, NoneType] = None, benchmark: bool = False, deterministic: bool = False, reload_dataloaders_every_epoch: bool = False, auto_lr_find: Union[bool, str] = False, replace_sampler_ddp: bool = True, terminate_on_nan: bool = False, auto_scale_batch_size: Union[str, bool] = False, prepare_data_per_node: bool = True, plugins: Union[pytorch_lightning.plugins.base_plugin.Plugin, str, list, NoneType] = None, amp_backend: str = 'native', amp_level: str = 'O2', distributed_backend: Union[str, NoneType] = None, automatic_optimization: Union[bool, NoneType] = None, move_metrics_to_cpu: bool = False, enable_pl_optimizer: bool = None, multiple_trainloader_mode: str = 'max_size_cycle', stochastic_weight_avg: bool = False)\n",
      " |  \n",
      " |  Helper class that provides a standard way to create an ABC using\n",
      " |  inheritance.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Trainer\n",
      " |      pytorch_lightning.trainer.properties.TrainerProperties\n",
      " |      pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin\n",
      " |      pytorch_lightning.trainer.model_hooks.TrainerModelHooksMixin\n",
      " |      pytorch_lightning.trainer.optimizers.TrainerOptimizersMixin\n",
      " |      pytorch_lightning.trainer.logging.TrainerLoggingMixin\n",
      " |      pytorch_lightning.trainer.training_tricks.TrainerTrainingTricksMixin\n",
      " |      pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin\n",
      " |      abc.ABC\n",
      " |      pytorch_lightning.trainer.deprecated_api.DeprecatedDistDeviceAttributes\n",
      " |      pytorch_lightning.trainer.deprecated_api.DeprecatedTrainerAttributes\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, logger: Union[pytorch_lightning.loggers.base.LightningLoggerBase, Iterable[pytorch_lightning.loggers.base.LightningLoggerBase], bool] = True, checkpoint_callback: bool = True, callbacks: Union[List[pytorch_lightning.callbacks.base.Callback], pytorch_lightning.callbacks.base.Callback, NoneType] = None, default_root_dir: Union[str, NoneType] = None, gradient_clip_val: float = 0, process_position: int = 0, num_nodes: int = 1, num_processes: int = 1, gpus: Union[int, str, List[int], NoneType] = None, auto_select_gpus: bool = False, tpu_cores: Union[int, str, List[int], NoneType] = None, log_gpu_memory: Union[str, NoneType] = None, progress_bar_refresh_rate: Union[int, NoneType] = None, overfit_batches: Union[int, float] = 0.0, track_grad_norm: Union[int, float, str] = -1, check_val_every_n_epoch: int = 1, fast_dev_run: Union[int, bool] = False, accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1, max_epochs: Union[int, NoneType] = None, min_epochs: Union[int, NoneType] = None, max_steps: Union[int, NoneType] = None, min_steps: Union[int, NoneType] = None, limit_train_batches: Union[int, float] = 1.0, limit_val_batches: Union[int, float] = 1.0, limit_test_batches: Union[int, float] = 1.0, limit_predict_batches: Union[int, float] = 1.0, val_check_interval: Union[int, float] = 1.0, flush_logs_every_n_steps: int = 100, log_every_n_steps: int = 50, accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator, NoneType] = None, sync_batchnorm: bool = False, precision: int = 32, weights_summary: Union[str, NoneType] = 'top', weights_save_path: Union[str, NoneType] = None, num_sanity_val_steps: int = 2, truncated_bptt_steps: Union[int, NoneType] = None, resume_from_checkpoint: Union[pathlib.Path, str, NoneType] = None, profiler: Union[pytorch_lightning.profiler.profilers.BaseProfiler, bool, str, NoneType] = None, benchmark: bool = False, deterministic: bool = False, reload_dataloaders_every_epoch: bool = False, auto_lr_find: Union[bool, str] = False, replace_sampler_ddp: bool = True, terminate_on_nan: bool = False, auto_scale_batch_size: Union[str, bool] = False, prepare_data_per_node: bool = True, plugins: Union[pytorch_lightning.plugins.base_plugin.Plugin, str, list, NoneType] = None, amp_backend: str = 'native', amp_level: str = 'O2', distributed_backend: Union[str, NoneType] = None, automatic_optimization: Union[bool, NoneType] = None, move_metrics_to_cpu: bool = False, enable_pl_optimizer: bool = None, multiple_trainloader_mode: str = 'max_size_cycle', stochastic_weight_avg: bool = False)\n",
      " |      Customize every aspect of training via flags\n",
      " |      \n",
      " |      Args:\n",
      " |      \n",
      " |          accelerator: Previously known as distributed_backend (dp, ddp, ddp2, etc...).\n",
      " |              Can also take in an accelerator object for custom hardware.\n",
      " |      \n",
      " |          accumulate_grad_batches: Accumulates grads every k batches or as set up in the dict.\n",
      " |      \n",
      " |          amp_backend: The mixed precision backend to use (\"native\" or \"apex\")\n",
      " |      \n",
      " |          amp_level: The optimization level to use (O1, O2, etc...).\n",
      " |      \n",
      " |          auto_lr_find: If set to True, will make trainer.tune() run a learning rate finder,\n",
      " |              trying to optimize initial learning for faster convergence. trainer.tune() method will\n",
      " |              set the suggested learning rate in self.lr or self.learning_rate in the LightningModule.\n",
      " |              To use a different key set a string instead of True with the key name.\n",
      " |      \n",
      " |          auto_scale_batch_size: If set to True, will `initially` run a batch size\n",
      " |              finder trying to find the largest batch size that fits into memory.\n",
      " |              The result will be stored in self.batch_size in the LightningModule.\n",
      " |              Additionally, can be set to either `power` that estimates the batch size through\n",
      " |              a power search or `binsearch` that estimates the batch size through a binary search.\n",
      " |      \n",
      " |          auto_select_gpus: If enabled and `gpus` is an integer, pick available\n",
      " |              gpus automatically. This is especially useful when\n",
      " |              GPUs are configured to be in \"exclusive mode\", such\n",
      " |              that only one process at a time can access them.\n",
      " |      \n",
      " |          benchmark: If true enables cudnn.benchmark.\n",
      " |      \n",
      " |          callbacks: Add a callback or list of callbacks.\n",
      " |      \n",
      " |          checkpoint_callback: If ``True``, enable checkpointing.\n",
      " |              It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`. Default: ``True``.\n",
      " |      \n",
      " |              .. warning:: Passing a ModelCheckpoint instance to this argument is deprecated since\n",
      " |                  v1.1 and will be unsupported from v1.3. Use `callbacks` argument instead.\n",
      " |      \n",
      " |          check_val_every_n_epoch: Check val every n train epochs.\n",
      " |      \n",
      " |          default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
      " |              Default: ``os.getcwd()``.\n",
      " |              Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      " |      \n",
      " |          deterministic: If true enables cudnn.deterministic.\n",
      " |      \n",
      " |          distributed_backend: deprecated. Please use 'accelerator'\n",
      " |      \n",
      " |          fast_dev_run: runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
      " |              of train, val and test to find any bugs (ie: a sort of unit test).\n",
      " |      \n",
      " |          flush_logs_every_n_steps: How often to flush logs to disk (defaults to every 100 steps).\n",
      " |      \n",
      " |          gpus: number of gpus to train on (int) or which GPUs to train on (list or str) applied per node\n",
      " |      \n",
      " |          gradient_clip_val: 0 means don't clip.\n",
      " |      \n",
      " |          limit_train_batches: How much of training dataset to check (floats = percent, int = num_batches)\n",
      " |      \n",
      " |          limit_val_batches: How much of validation dataset to check (floats = percent, int = num_batches)\n",
      " |      \n",
      " |          limit_test_batches: How much of test dataset to check (floats = percent, int = num_batches)\n",
      " |      \n",
      " |          logger: Logger (or iterable collection of loggers) for experiment tracking.\n",
      " |      \n",
      " |          log_gpu_memory: None, 'min_max', 'all'. Might slow performance\n",
      " |      \n",
      " |          log_every_n_steps: How often to log within steps (defaults to every 50 steps).\n",
      " |      \n",
      " |          automatic_optimization: If False you are responsible for calling .backward, .step, zero_grad\n",
      " |              in LightningModule. This argument has been moved to LightningModule. It is deprecated\n",
      " |              here in v1.1 and will be removed in v1.3.\n",
      " |      \n",
      " |          prepare_data_per_node: If True, each LOCAL_RANK=0 will call prepare data.\n",
      " |              Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data\n",
      " |      \n",
      " |          process_position: orders the progress bar when running multiple models on same machine.\n",
      " |      \n",
      " |          progress_bar_refresh_rate: How often to refresh progress bar (in steps). Value ``0`` disables progress bar.\n",
      " |              Ignored when a custom progress bar is passed to :paramref:`~Trainer.callbacks`. Default: None, means\n",
      " |              a suitable value will be chosen based on the environment (terminal, Google COLAB, etc.).\n",
      " |      \n",
      " |          profiler: To profile individual steps during training and assist in identifying bottlenecks. Passing bool\n",
      " |              value is deprecated in v1.1 and will be removed in v1.3.\n",
      " |      \n",
      " |          overfit_batches: Overfit a percent of training data (float) or a set number of batches (int). Default: 0.0\n",
      " |      \n",
      " |          plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
      " |      \n",
      " |          precision: Full precision (32), half precision (16). Can be used on CPU, GPU or TPUs.\n",
      " |      \n",
      " |          max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
      " |              If both max_epochs and max_steps are not specified, defaults to ``max_epochs`` = 1000.\n",
      " |      \n",
      " |          min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
      " |              If both min_epochs and min_steps are not specified, defaults to ``min_epochs`` = 1.\n",
      " |      \n",
      " |          max_steps: Stop training after this number of steps. Disabled by default (None).\n",
      " |      \n",
      " |          min_steps: Force training for at least these number of steps. Disabled by default (None).\n",
      " |      \n",
      " |          num_nodes: number of GPU nodes for distributed training.\n",
      " |      \n",
      " |          num_processes: number of processes for distributed training with distributed_backend=\"ddp_cpu\"\n",
      " |      \n",
      " |          num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
      " |              Set it to `-1` to run all batches in all validation dataloaders. Default: 2\n",
      " |      \n",
      " |          reload_dataloaders_every_epoch: Set to True to reload dataloaders every epoch.\n",
      " |      \n",
      " |          replace_sampler_ddp: Explicitly enables or disables sampler replacement. If not specified this\n",
      " |              will toggled automatically when DDP is used. By default it will add ``shuffle=True`` for\n",
      " |              train sampler and ``shuffle=False`` for val/test sampler. If you want to customize it,\n",
      " |              you can set ``replace_sampler_ddp=False`` and add your own distributed sampler.\n",
      " |      \n",
      " |          resume_from_checkpoint: Path/URL of the checkpoint from which training is resumed. If there is\n",
      " |              no checkpoint file at the path, start from scratch. If resuming from mid-epoch checkpoint,\n",
      " |              training will start from the beginning of the next epoch.\n",
      " |      \n",
      " |          sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
      " |      \n",
      " |          terminate_on_nan: If set to True, will terminate training (by raising a `ValueError`) at the\n",
      " |              end of each training batch, if any of the parameters or the loss are NaN or +/-inf.\n",
      " |      \n",
      " |          tpu_cores: How many TPU cores to train on (1 or 8) / Single TPU to train on [1]\n",
      " |      \n",
      " |          track_grad_norm: -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm.\n",
      " |      \n",
      " |          truncated_bptt_steps: Truncated back prop breaks performs backprop every k steps of much longer\n",
      " |              sequence.\n",
      " |      \n",
      " |          val_check_interval: How often to check the validation set. Use float to check within a training epoch,\n",
      " |              use int to check every n steps (batches).\n",
      " |      \n",
      " |          weights_summary: Prints a summary of the weights when training begins.\n",
      " |      \n",
      " |          weights_save_path: Where to save weights if specified. Will override default_root_dir\n",
      " |              for checkpoints only. Use this if for whatever reason you need the checkpoints\n",
      " |              stored in a different place than the logs written in `default_root_dir`.\n",
      " |              Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      " |              Defaults to `default_root_dir`.\n",
      " |      \n",
      " |          move_metrics_to_cpu: Whether to force internal logged metrics to be moved to cpu.\n",
      " |              This can save some gpu memory, but can make training slower. Use with attention.\n",
      " |      \n",
      " |          enable_pl_optimizer: If True, each optimizer will be wrapped by\n",
      " |              `pytorch_lightning.core.optimizer.LightningOptimizer`. It allows Lightning to\n",
      " |              handle AMP, TPU, accumulated_gradients, etc.\n",
      " |              .. warning:: Currently deprecated and it will be removed in v1.3\n",
      " |      \n",
      " |          multiple_trainloader_mode: How to loop over the datasets when there are multiple train loaders.\n",
      " |              In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed,\n",
      " |              and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets\n",
      " |              reload when reaching the minimum length of datasets.\n",
      " |      \n",
      " |          stochastic_weight_avg: Whether to use `Stochastic Weight Averaging (SWA)\n",
      " |              <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/>_`\n",
      " |  \n",
      " |  call_hook(self, hook_name, *args, **kwargs)\n",
      " |  \n",
      " |  call_setup_hook(self, model)\n",
      " |  \n",
      " |  dispatch(self)\n",
      " |  \n",
      " |  fit(self, model: pytorch_lightning.core.lightning.LightningModule, train_dataloader: Union[torch.utils.data.dataloader.DataLoader, NoneType] = None, val_dataloaders: Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader], NoneType] = None, datamodule: Union[pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None)\n",
      " |      Runs the full optimization routine.\n",
      " |      \n",
      " |      Args:\n",
      " |          datamodule: A instance of :class:`LightningDataModule`.\n",
      " |      \n",
      " |          model: Model to fit.\n",
      " |      \n",
      " |          train_dataloader: A Pytorch DataLoader with training samples. If the model has\n",
      " |              a predefined train_dataloader method this will be skipped.\n",
      " |      \n",
      " |          val_dataloaders: Either a single Pytorch Dataloader or a list of them, specifying validation samples.\n",
      " |              If the model has a predefined val_dataloaders method this will be skipped\n",
      " |  \n",
      " |  post_dispatch(self)\n",
      " |  \n",
      " |  pre_dispatch(self)\n",
      " |  \n",
      " |  predict(self, model: Union[pytorch_lightning.core.lightning.LightningModule, NoneType] = None, dataloaders: Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader], NoneType] = None, datamodule: Union[pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None)\n",
      " |      Separates from fit to make sure you never run on your predictions set until you want to.\n",
      " |      \n",
      " |      This will call the model forward function to compute predictions.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The model to predict on.\n",
      " |      \n",
      " |          dataloaders: Either a single\n",
      " |              Pytorch Dataloader or a list of them, specifying inference samples.\n",
      " |      \n",
      " |          datamodule: A instance of :class:`LightningDataModule`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\n",
      " |  \n",
      " |  run_evaluation(self, max_batches=None, on_epoch=False)\n",
      " |  \n",
      " |  run_predict(self)\n",
      " |  \n",
      " |  run_sanity_check(self, ref_model)\n",
      " |  \n",
      " |  run_test(self)\n",
      " |  \n",
      " |  run_train(self)\n",
      " |  \n",
      " |  setup_trainer(self, model: pytorch_lightning.core.lightning.LightningModule)\n",
      " |      Sanity check a few things before starting actual training or testing.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The model to run sanity test on.\n",
      " |  \n",
      " |  test(self, model: Union[pytorch_lightning.core.lightning.LightningModule, NoneType] = None, test_dataloaders: Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader], NoneType] = None, ckpt_path: Union[str, NoneType] = 'best', verbose: bool = True, datamodule: Union[pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None)\n",
      " |      Separates from fit to make sure you never run on your test set until you want to.\n",
      " |      \n",
      " |      Args:\n",
      " |          ckpt_path: Either ``best`` or path to the checkpoint you wish to test.\n",
      " |              If ``None``, use the weights from the last epoch to test. Default to ``best``.\n",
      " |      \n",
      " |          datamodule: A instance of :class:`LightningDataModule`.\n",
      " |      \n",
      " |          model: The model to test.\n",
      " |      \n",
      " |          test_dataloaders: Either a single\n",
      " |              Pytorch Dataloader or a list of them, specifying validation samples.\n",
      " |      \n",
      " |          verbose: If True, prints the test results\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns a list of dictionaries, one for each test dataloader containing their respective metrics.\n",
      " |  \n",
      " |  track_output_for_epoch_end(self, outputs, output)\n",
      " |  \n",
      " |  train_or_test_or_predict(self)\n",
      " |  \n",
      " |  tune(self, model: pytorch_lightning.core.lightning.LightningModule, train_dataloader: Union[torch.utils.data.dataloader.DataLoader, NoneType] = None, val_dataloaders: Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader], NoneType] = None, datamodule: Union[pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None)\n",
      " |      Runs routines to tune hyperparameters before training.\n",
      " |      \n",
      " |      Args:\n",
      " |          datamodule: A instance of :class:`LightningDataModule`.\n",
      " |      \n",
      " |          model: Model to tune.\n",
      " |      \n",
      " |          train_dataloader: A Pytorch DataLoader with training samples. If the model has\n",
      " |              a predefined train_dataloader method this will be skipped.\n",
      " |      \n",
      " |          val_dataloaders: Either a single Pytorch Dataloader or a list of them, specifying validation samples.\n",
      " |              If the model has a predefined val_dataloaders method this will be skipped\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  evaluating\n",
      " |  \n",
      " |  predicting\n",
      " |  \n",
      " |  testing\n",
      " |  \n",
      " |  training\n",
      " |  \n",
      " |  tuning\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.properties.TrainerProperties:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      # TODO: refactor this so that it can be done in LightningOptimizer\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  save_checkpoint(self, filepath, weights_only: bool = False) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pytorch_lightning.trainer.properties.TrainerProperties:\n",
      " |  \n",
      " |  add_argparse_args(parent_parser: argparse.ArgumentParser) -> argparse.ArgumentParser from abc.ABCMeta\n",
      " |  \n",
      " |  default_attributes() -> dict from abc.ABCMeta\n",
      " |  \n",
      " |  from_argparse_args(args: Union[argparse.Namespace, argparse.ArgumentParser], **kwargs) -> '_T' from abc.ABCMeta\n",
      " |  \n",
      " |  get_deprecated_arg_names() -> List from abc.ABCMeta\n",
      " |      Returns a list with deprecated Trainer arguments.\n",
      " |  \n",
      " |  match_env_arguments() -> argparse.Namespace from abc.ABCMeta\n",
      " |  \n",
      " |  parse_argparser(arg_parser: Union[argparse.ArgumentParser, argparse.Namespace]) -> argparse.Namespace from abc.ABCMeta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pytorch_lightning.trainer.properties.TrainerProperties:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  accelerator\n",
      " |  \n",
      " |  amp_backend\n",
      " |  \n",
      " |  callback_metrics\n",
      " |  \n",
      " |  checkpoint_callback\n",
      " |      The first :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint`\n",
      " |      callback in the Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      " |  \n",
      " |  checkpoint_callbacks\n",
      " |      A list of all instances of :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint`\n",
      " |      found in the Trainer.callbacks list.\n",
      " |  \n",
      " |  data_parallel\n",
      " |  \n",
      " |  data_parallel_device_ids\n",
      " |  \n",
      " |  default_root_dir\n",
      " |      The default location to save artifacts of loggers, checkpoints etc.\n",
      " |      It is used as a fallback if logger or checkpoint callback do not define specific save paths.\n",
      " |  \n",
      " |  disable_validation\n",
      " |      Check if validation is disabled during training.\n",
      " |  \n",
      " |  distributed_backend\n",
      " |  \n",
      " |  distributed_sampler_kwargs\n",
      " |  \n",
      " |  early_stopping_callback\n",
      " |      The first :class:`~pytorch_lightning.callbacks.early_stopping.EarlyStopping`\n",
      " |      callback in the Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      " |  \n",
      " |  early_stopping_callbacks\n",
      " |      A list of all instances of :class:`~pytorch_lightning.callbacks.early_stopping.EarlyStopping`\n",
      " |      found in the Trainer.callbacks list.\n",
      " |  \n",
      " |  enable_validation\n",
      " |      Check if we should run validation during training.\n",
      " |  \n",
      " |  global_rank\n",
      " |  \n",
      " |  gpus\n",
      " |  \n",
      " |  is_global_zero\n",
      " |  \n",
      " |  lightning_module\n",
      " |  \n",
      " |  lightning_optimizers\n",
      " |  \n",
      " |  local_rank\n",
      " |  \n",
      " |  log_dir\n",
      " |  \n",
      " |  logged_metrics\n",
      " |  \n",
      " |  lr_schedulers\n",
      " |  \n",
      " |  model\n",
      " |      The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.\n",
      " |      To access the pure LightningModule, use\n",
      " |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.lightning_module` instead.\n",
      " |  \n",
      " |  node_rank\n",
      " |  \n",
      " |  num_gpus\n",
      " |  \n",
      " |  num_nodes\n",
      " |  \n",
      " |  num_processes\n",
      " |  \n",
      " |  optimizer_frequencies\n",
      " |  \n",
      " |  optimizers\n",
      " |  \n",
      " |  precision\n",
      " |  \n",
      " |  precision_plugin\n",
      " |  \n",
      " |  progress_bar_callback\n",
      " |  \n",
      " |  progress_bar_dict\n",
      " |      Read-only for progress bar metrics.\n",
      " |  \n",
      " |  progress_bar_metrics\n",
      " |  \n",
      " |  root_gpu\n",
      " |  \n",
      " |  scaler\n",
      " |  \n",
      " |  slurm_job_id\n",
      " |  \n",
      " |  state\n",
      " |  \n",
      " |  tpu_cores\n",
      " |  \n",
      " |  training_type_plugin\n",
      " |  \n",
      " |  use_amp\n",
      " |  \n",
      " |  weights_save_path\n",
      " |      The default root location to save weights (checkpoints), e.g., when the\n",
      " |      :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` does not define a file path.\n",
      " |  \n",
      " |  world_size\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pytorch_lightning.trainer.properties.TrainerProperties:\n",
      " |  \n",
      " |  __annotations__ = {'_default_root_dir': <class 'str'>, '_progress_bar_...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin:\n",
      " |  \n",
      " |  on_after_backward(self)\n",
      " |      Called after loss.backward() and before optimizers do anything.\n",
      " |  \n",
      " |  on_batch_end(self)\n",
      " |      Called when the training batch ends.\n",
      " |  \n",
      " |  on_batch_start(self)\n",
      " |      Called when the training batch begins.\n",
      " |  \n",
      " |  on_before_accelerator_backend_setup(self, model)\n",
      " |      Called in the beginning of fit and test\n",
      " |  \n",
      " |  on_before_zero_grad(self, optimizer)\n",
      " |      Called after optimizer.step() and before optimizer.zero_grad().\n",
      " |  \n",
      " |  on_epoch_end(self)\n",
      " |      Called when the epoch ends.\n",
      " |  \n",
      " |  on_epoch_start(self)\n",
      " |      Called when the epoch begins.\n",
      " |  \n",
      " |  on_fit_end(self)\n",
      " |      Called when the trainer initialization begins, model has not yet been set.\n",
      " |  \n",
      " |  on_fit_start(self)\n",
      " |      Called when the trainer initialization begins, model has not yet been set.\n",
      " |  \n",
      " |  on_init_end(self)\n",
      " |      Called when the trainer initialization ends, model has not yet been set.\n",
      " |  \n",
      " |  on_init_start(self)\n",
      " |      Called when the trainer initialization begins, model has not yet been set.\n",
      " |  \n",
      " |  on_keyboard_interrupt(self)\n",
      " |      Called when the training is interrupted by KeyboardInterrupt.\n",
      " |  \n",
      " |  on_load_checkpoint(self, checkpoint)\n",
      " |      Called when loading a model checkpoint.\n",
      " |  \n",
      " |  on_pretrain_routine_end(self, model)\n",
      " |      Called when the train ends.\n",
      " |  \n",
      " |  on_pretrain_routine_start(self, model)\n",
      " |      Called when the train begins.\n",
      " |  \n",
      " |  on_sanity_check_end(self)\n",
      " |      Called when the validation sanity check ends.\n",
      " |  \n",
      " |  on_sanity_check_start(self)\n",
      " |      Called when the validation sanity check starts.\n",
      " |  \n",
      " |  on_save_checkpoint(self)\n",
      " |      Called when saving a model checkpoint.\n",
      " |  \n",
      " |  on_test_batch_end(self, outputs, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the test batch ends.\n",
      " |  \n",
      " |  on_test_batch_start(self, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the test batch begins.\n",
      " |  \n",
      " |  on_test_end(self)\n",
      " |      Called when the test ends.\n",
      " |  \n",
      " |  on_test_epoch_end(self)\n",
      " |      Called when the epoch ends.\n",
      " |  \n",
      " |  on_test_epoch_start(self)\n",
      " |      Called when the epoch begins.\n",
      " |  \n",
      " |  on_test_start(self)\n",
      " |      Called when the test begins.\n",
      " |  \n",
      " |  on_train_batch_end(self, outputs, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the training batch ends.\n",
      " |  \n",
      " |  on_train_batch_start(self, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the training batch begins.\n",
      " |  \n",
      " |  on_train_end(self)\n",
      " |      Called when the train ends.\n",
      " |  \n",
      " |  on_train_epoch_end(self, outputs)\n",
      " |      Called when the epoch ends.\n",
      " |  \n",
      " |  on_train_epoch_start(self)\n",
      " |      Called when the epoch begins.\n",
      " |  \n",
      " |  on_train_start(self)\n",
      " |      Called when the train begins.\n",
      " |  \n",
      " |  on_validation_batch_end(self, outputs, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the validation batch ends.\n",
      " |  \n",
      " |  on_validation_batch_start(self, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the validation batch begins.\n",
      " |  \n",
      " |  on_validation_end(self)\n",
      " |      Called when the validation loop ends.\n",
      " |  \n",
      " |  on_validation_epoch_end(self)\n",
      " |      Called when the epoch ends.\n",
      " |  \n",
      " |  on_validation_epoch_start(self)\n",
      " |      Called when the epoch begins.\n",
      " |  \n",
      " |  on_validation_start(self)\n",
      " |      Called when the validation loop begins.\n",
      " |  \n",
      " |  setup(self, model, stage: str)\n",
      " |      Called in the beginning of fit and test\n",
      " |  \n",
      " |  teardown(self, stage: str)\n",
      " |      Called at the end of fit and test\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin:\n",
      " |  \n",
      " |  callbacks = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.model_hooks.TrainerModelHooksMixin:\n",
      " |  \n",
      " |  has_arg(self, f_name, arg_name)\n",
      " |  \n",
      " |  is_function_implemented(self, f_name, model=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.optimizers.TrainerOptimizersMixin:\n",
      " |  \n",
      " |  configure_schedulers(self, schedulers: list, monitor: Union[str, NoneType] = None)\n",
      " |  \n",
      " |  convert_to_lightning_optimizers(self)\n",
      " |  \n",
      " |  init_optimizers(self, model: pytorch_lightning.core.lightning.LightningModule) -> Tuple[List, List, List]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.logging.TrainerLoggingMixin:\n",
      " |  \n",
      " |  metrics_to_scalars(self, metrics)\n",
      " |  \n",
      " |  process_dict_result(self, output, train=False)\n",
      " |      Reduces output according to the training mode.\n",
      " |      \n",
      " |      Separates loss from logging and progress bar metrics\n",
      " |  \n",
      " |  reduce_distributed_output(self, output, num_gpus)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.training_tricks.TrainerTrainingTricksMixin:\n",
      " |  \n",
      " |  detect_nan_tensors(self, loss: torch.Tensor) -> None\n",
      " |  \n",
      " |  print_nan_gradients(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin:\n",
      " |  \n",
      " |  auto_add_sampler(self, dataloader: torch.utils.data.dataloader.DataLoader, shuffle: bool) -> torch.utils.data.dataloader.DataLoader\n",
      " |  \n",
      " |  replace_sampler(self, dataloader, sampler)\n",
      " |  \n",
      " |  request_dataloader(self, dataloader_fx: Callable) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Handles downloading data in the GPU or TPU case.\n",
      " |      \n",
      " |      Args:\n",
      " |          dataloader_fx: The bound dataloader getter\n",
      " |      \n",
      " |      Returns:\n",
      " |          The dataloader\n",
      " |  \n",
      " |  reset_predict_dataloader(self, model) -> None\n",
      " |      Resets the predict dataloader and determines the number of batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The current `LightningModule`\n",
      " |  \n",
      " |  reset_test_dataloader(self, model) -> None\n",
      " |      Resets the test dataloader and determines the number of batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The current `LightningModule`\n",
      " |  \n",
      " |  reset_train_dataloader(self, model: pytorch_lightning.core.lightning.LightningModule) -> None\n",
      " |      Resets the train dataloader and initialises required variables\n",
      " |      (number of batches, when to validate, etc.).\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The current `LightningModule`\n",
      " |  \n",
      " |  reset_val_dataloader(self, model: pytorch_lightning.core.lightning.LightningModule) -> None\n",
      " |      Resets the validation dataloader and determines the number of batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The current `LightningModule`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pytorch_lightning.trainer.deprecated_api.DeprecatedDistDeviceAttributes:\n",
      " |  \n",
      " |  on_cpu\n",
      " |  \n",
      " |  on_gpu\n",
      " |  \n",
      " |  on_tpu\n",
      " |  \n",
      " |  use_ddp\n",
      " |  \n",
      " |  use_ddp2\n",
      " |  \n",
      " |  use_dp\n",
      " |  \n",
      " |  use_horovod\n",
      " |  \n",
      " |  use_single_gpu\n",
      " |  \n",
      " |  use_tpu\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.deprecated_api.DeprecatedTrainerAttributes:\n",
      " |  \n",
      " |  get_model(self) -> pytorch_lightning.core.lightning.LightningModule\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pytorch_lightning.trainer.deprecated_api.DeprecatedTrainerAttributes:\n",
      " |  \n",
      " |  accelerator_backend\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "help(pl.Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FunctionCollection as fc\n",
    "import importlib\n",
    "fc = importlib.reload(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AzZe_target_constructor(dataset, transformer):\n",
    "    az_ze = dataset.data.y.view(-1,10,1)[:,[8,9]]\n",
    "    tfs = transformer\n",
    "    \n",
    "    az = torch.tensor(tfs['truth']['azimuth'].inverse_transform(az_ze[:,0]),dtype=torch.float)\n",
    "    ze = torch.tensor(tfs['truth']['zenith'].inverse_transform(az_ze[:,1]),dtype=torch.float) #the range seems to be about [0,pi/2]?. Makes sense, Muons come from the atmosphere\n",
    "    \n",
    "    dataset.data.y = torch.cat([az,ze],dim=1).flatten()\n",
    "    dataset.slices['y'] = np.arange(0,len(dataset.data.y)+1, 2)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-22 11:00:39.782510: loading data..\n",
      "2021-02-22 11:00:39.922462: executing target constructor..\n",
      "2021-02-22 11:00:39.927459: executing feature constructor..\n",
      "2021-02-22 11:00:47.891493: shuffling dataset..\n",
      "2021-02-22 11:00:47.919482: defining dataloaders..\n",
      "2021-02-22 11:00:47.920482: Done!\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/jv97/Desktop/github/Neutrino-Machine-Learning/datasets\"\n",
    "filename = \"muon_100k_set11_SRT.pt\"\n",
    "transformer = pd.read_pickle(path + \"/transformers.pkl\")\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "dataset, train_loader, test_loader, val_loader = fc.dataset_preparator(name = filename,\n",
    "                                                                       path = path,\n",
    "                                                                       transformer = transformer,\n",
    "                                                                       tc = AzZe_target_constructor,\n",
    "                                                                       fc = fc.custom_feature_constructor,\n",
    "                                                                       shuffle = True,\n",
    "                                                                       TrTV_split = (0.9,0.9,1),\n",
    "                                                                       batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember all accuracies are positive and defined to go towards 0 in the optimal case.\n"
     ]
    }
   ],
   "source": [
    "import Model_tmp as M\n",
    "M = importlib.reload(M)\n",
    "\n",
    "run_name = 'Test run - delete me'\n",
    "\n",
    "args = {'N_edge_feats': 6,\n",
    "        'N_dom_feats': 5,\n",
    "        'N_targets': 3,\n",
    "        'N_metalayers': 5,\n",
    "        'N_hcs': 16,\n",
    "        'wandb_activated': True,\n",
    "        'lr': 1e-3,\n",
    "        'batch_size': batch_size}\n",
    "\n",
    "model = M.Load_model('Spherical_NLLH',args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "early_stop_callback = pl.callbacks.early_stopping.EarlyStopping(monitor='Val Acc', \n",
    "                                                                min_delta=0.00, \n",
    "                                                                patience=3, \n",
    "                                                                verbose=False, \n",
    "                                                                mode='min')\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(dirpath = os.getcwd()+'\\\\checkpoints',\n",
    "                                                   filename = '{epoch}-{Val Acc:.3f}',\n",
    "                                                   save_top_k = 1,\n",
    "                                                   verbose = True,\n",
    "                                                   monitor = 'Val Acc',\n",
    "                                                   mode = 'min',\n",
    "                                                   prefix = run_name)\n",
    "\n",
    "lr_logger = pl.callbacks.lr_monitor.LearningRateMonitor(logging_interval = 'epoch')\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(name = run_name,\n",
    "                           project = 'Neutrino-Machine-Learning')\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, #-1 for all gpus\n",
    "                     max_epochs=5,\n",
    "                     auto_lr_find = True,\n",
    "                     log_every_n_steps = 5,\n",
    "                     callbacks=[early_stop_callback, checkpoint_callback, lr_logger], \n",
    "                     logger = wandb_logger if args['wandb_activated'] else False)\n",
    "\n",
    "# if args['wandb_activated']:\n",
    "#     wandb_logger.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jv97\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:50: UserWarning: you defined a validation_step but have no val_dataloader. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name              | Type       | Params\n",
      "-------------------------------------------------\n",
      "0 | act               | SiLU       | 0     \n",
      "1 | x_encoder         | Linear     | 480   \n",
      "2 | edge_attr_encoder | Linear     | 112   \n",
      "3 | u_encoder         | Linear     | 1.9 K \n",
      "4 | ops               | ModuleList | 399 K \n",
      "5 | decoders          | ModuleList | 75.4 K\n",
      "6 | decoder           | Linear     | 99    \n",
      "-------------------------------------------------\n",
      "477 K     Trainable params\n",
      "0         Non-trainable params\n",
      "477 K     Total params\n",
      "1.908     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb3bb96e1cf04cf795809f123cfcf95f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Finding best initial lr', max=500.0, style=ProgressStyle(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LR finder stopped early due to diverging loss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restored states from the checkpoint file at C:\\Users\\jv97\\Desktop\\github\\Neutrino-Machine-Learning\\lr_find_temp_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "lr_finder = trainer.tuner.lr_find(model,train_loader,num_training=500,early_stop_threshold=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Trainer in module pytorch_lightning.trainer.trainer:\n",
      "\n",
      "class Trainer(pytorch_lightning.trainer.properties.TrainerProperties, pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin, pytorch_lightning.trainer.model_hooks.TrainerModelHooksMixin, pytorch_lightning.trainer.optimizers.TrainerOptimizersMixin, pytorch_lightning.trainer.logging.TrainerLoggingMixin, pytorch_lightning.trainer.training_tricks.TrainerTrainingTricksMixin, pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin, pytorch_lightning.trainer.deprecated_api.DeprecatedDistDeviceAttributes, pytorch_lightning.trainer.deprecated_api.DeprecatedTrainerAttributes)\n",
      " |  Trainer(logger: Union[pytorch_lightning.loggers.base.LightningLoggerBase, Iterable[pytorch_lightning.loggers.base.LightningLoggerBase], bool] = True, checkpoint_callback: bool = True, callbacks: Union[List[pytorch_lightning.callbacks.base.Callback], pytorch_lightning.callbacks.base.Callback, NoneType] = None, default_root_dir: Union[str, NoneType] = None, gradient_clip_val: float = 0, process_position: int = 0, num_nodes: int = 1, num_processes: int = 1, gpus: Union[int, str, List[int], NoneType] = None, auto_select_gpus: bool = False, tpu_cores: Union[int, str, List[int], NoneType] = None, log_gpu_memory: Union[str, NoneType] = None, progress_bar_refresh_rate: Union[int, NoneType] = None, overfit_batches: Union[int, float] = 0.0, track_grad_norm: Union[int, float, str] = -1, check_val_every_n_epoch: int = 1, fast_dev_run: Union[int, bool] = False, accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1, max_epochs: Union[int, NoneType] = None, min_epochs: Union[int, NoneType] = None, max_steps: Union[int, NoneType] = None, min_steps: Union[int, NoneType] = None, limit_train_batches: Union[int, float] = 1.0, limit_val_batches: Union[int, float] = 1.0, limit_test_batches: Union[int, float] = 1.0, limit_predict_batches: Union[int, float] = 1.0, val_check_interval: Union[int, float] = 1.0, flush_logs_every_n_steps: int = 100, log_every_n_steps: int = 50, accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator, NoneType] = None, sync_batchnorm: bool = False, precision: int = 32, weights_summary: Union[str, NoneType] = 'top', weights_save_path: Union[str, NoneType] = None, num_sanity_val_steps: int = 2, truncated_bptt_steps: Union[int, NoneType] = None, resume_from_checkpoint: Union[pathlib.Path, str, NoneType] = None, profiler: Union[pytorch_lightning.profiler.profilers.BaseProfiler, bool, str, NoneType] = None, benchmark: bool = False, deterministic: bool = False, reload_dataloaders_every_epoch: bool = False, auto_lr_find: Union[bool, str] = False, replace_sampler_ddp: bool = True, terminate_on_nan: bool = False, auto_scale_batch_size: Union[str, bool] = False, prepare_data_per_node: bool = True, plugins: Union[pytorch_lightning.plugins.base_plugin.Plugin, str, list, NoneType] = None, amp_backend: str = 'native', amp_level: str = 'O2', distributed_backend: Union[str, NoneType] = None, automatic_optimization: Union[bool, NoneType] = None, move_metrics_to_cpu: bool = False, enable_pl_optimizer: bool = None, multiple_trainloader_mode: str = 'max_size_cycle', stochastic_weight_avg: bool = False)\n",
      " |  \n",
      " |  Helper class that provides a standard way to create an ABC using\n",
      " |  inheritance.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Trainer\n",
      " |      pytorch_lightning.trainer.properties.TrainerProperties\n",
      " |      pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin\n",
      " |      pytorch_lightning.trainer.model_hooks.TrainerModelHooksMixin\n",
      " |      pytorch_lightning.trainer.optimizers.TrainerOptimizersMixin\n",
      " |      pytorch_lightning.trainer.logging.TrainerLoggingMixin\n",
      " |      pytorch_lightning.trainer.training_tricks.TrainerTrainingTricksMixin\n",
      " |      pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin\n",
      " |      abc.ABC\n",
      " |      pytorch_lightning.trainer.deprecated_api.DeprecatedDistDeviceAttributes\n",
      " |      pytorch_lightning.trainer.deprecated_api.DeprecatedTrainerAttributes\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, logger: Union[pytorch_lightning.loggers.base.LightningLoggerBase, Iterable[pytorch_lightning.loggers.base.LightningLoggerBase], bool] = True, checkpoint_callback: bool = True, callbacks: Union[List[pytorch_lightning.callbacks.base.Callback], pytorch_lightning.callbacks.base.Callback, NoneType] = None, default_root_dir: Union[str, NoneType] = None, gradient_clip_val: float = 0, process_position: int = 0, num_nodes: int = 1, num_processes: int = 1, gpus: Union[int, str, List[int], NoneType] = None, auto_select_gpus: bool = False, tpu_cores: Union[int, str, List[int], NoneType] = None, log_gpu_memory: Union[str, NoneType] = None, progress_bar_refresh_rate: Union[int, NoneType] = None, overfit_batches: Union[int, float] = 0.0, track_grad_norm: Union[int, float, str] = -1, check_val_every_n_epoch: int = 1, fast_dev_run: Union[int, bool] = False, accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1, max_epochs: Union[int, NoneType] = None, min_epochs: Union[int, NoneType] = None, max_steps: Union[int, NoneType] = None, min_steps: Union[int, NoneType] = None, limit_train_batches: Union[int, float] = 1.0, limit_val_batches: Union[int, float] = 1.0, limit_test_batches: Union[int, float] = 1.0, limit_predict_batches: Union[int, float] = 1.0, val_check_interval: Union[int, float] = 1.0, flush_logs_every_n_steps: int = 100, log_every_n_steps: int = 50, accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator, NoneType] = None, sync_batchnorm: bool = False, precision: int = 32, weights_summary: Union[str, NoneType] = 'top', weights_save_path: Union[str, NoneType] = None, num_sanity_val_steps: int = 2, truncated_bptt_steps: Union[int, NoneType] = None, resume_from_checkpoint: Union[pathlib.Path, str, NoneType] = None, profiler: Union[pytorch_lightning.profiler.profilers.BaseProfiler, bool, str, NoneType] = None, benchmark: bool = False, deterministic: bool = False, reload_dataloaders_every_epoch: bool = False, auto_lr_find: Union[bool, str] = False, replace_sampler_ddp: bool = True, terminate_on_nan: bool = False, auto_scale_batch_size: Union[str, bool] = False, prepare_data_per_node: bool = True, plugins: Union[pytorch_lightning.plugins.base_plugin.Plugin, str, list, NoneType] = None, amp_backend: str = 'native', amp_level: str = 'O2', distributed_backend: Union[str, NoneType] = None, automatic_optimization: Union[bool, NoneType] = None, move_metrics_to_cpu: bool = False, enable_pl_optimizer: bool = None, multiple_trainloader_mode: str = 'max_size_cycle', stochastic_weight_avg: bool = False)\n",
      " |      Customize every aspect of training via flags\n",
      " |      \n",
      " |      Args:\n",
      " |      \n",
      " |          accelerator: Previously known as distributed_backend (dp, ddp, ddp2, etc...).\n",
      " |              Can also take in an accelerator object for custom hardware.\n",
      " |      \n",
      " |          accumulate_grad_batches: Accumulates grads every k batches or as set up in the dict.\n",
      " |      \n",
      " |          amp_backend: The mixed precision backend to use (\"native\" or \"apex\")\n",
      " |      \n",
      " |          amp_level: The optimization level to use (O1, O2, etc...).\n",
      " |      \n",
      " |          auto_lr_find: If set to True, will make trainer.tune() run a learning rate finder,\n",
      " |              trying to optimize initial learning for faster convergence. trainer.tune() method will\n",
      " |              set the suggested learning rate in self.lr or self.learning_rate in the LightningModule.\n",
      " |              To use a different key set a string instead of True with the key name.\n",
      " |      \n",
      " |          auto_scale_batch_size: If set to True, will `initially` run a batch size\n",
      " |              finder trying to find the largest batch size that fits into memory.\n",
      " |              The result will be stored in self.batch_size in the LightningModule.\n",
      " |              Additionally, can be set to either `power` that estimates the batch size through\n",
      " |              a power search or `binsearch` that estimates the batch size through a binary search.\n",
      " |      \n",
      " |          auto_select_gpus: If enabled and `gpus` is an integer, pick available\n",
      " |              gpus automatically. This is especially useful when\n",
      " |              GPUs are configured to be in \"exclusive mode\", such\n",
      " |              that only one process at a time can access them.\n",
      " |      \n",
      " |          benchmark: If true enables cudnn.benchmark.\n",
      " |      \n",
      " |          callbacks: Add a callback or list of callbacks.\n",
      " |      \n",
      " |          checkpoint_callback: If ``True``, enable checkpointing.\n",
      " |              It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`. Default: ``True``.\n",
      " |      \n",
      " |              .. warning:: Passing a ModelCheckpoint instance to this argument is deprecated since\n",
      " |                  v1.1 and will be unsupported from v1.3. Use `callbacks` argument instead.\n",
      " |      \n",
      " |          check_val_every_n_epoch: Check val every n train epochs.\n",
      " |      \n",
      " |          default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
      " |              Default: ``os.getcwd()``.\n",
      " |              Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      " |      \n",
      " |          deterministic: If true enables cudnn.deterministic.\n",
      " |      \n",
      " |          distributed_backend: deprecated. Please use 'accelerator'\n",
      " |      \n",
      " |          fast_dev_run: runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
      " |              of train, val and test to find any bugs (ie: a sort of unit test).\n",
      " |      \n",
      " |          flush_logs_every_n_steps: How often to flush logs to disk (defaults to every 100 steps).\n",
      " |      \n",
      " |          gpus: number of gpus to train on (int) or which GPUs to train on (list or str) applied per node\n",
      " |      \n",
      " |          gradient_clip_val: 0 means don't clip.\n",
      " |      \n",
      " |          limit_train_batches: How much of training dataset to check (floats = percent, int = num_batches)\n",
      " |      \n",
      " |          limit_val_batches: How much of validation dataset to check (floats = percent, int = num_batches)\n",
      " |      \n",
      " |          limit_test_batches: How much of test dataset to check (floats = percent, int = num_batches)\n",
      " |      \n",
      " |          logger: Logger (or iterable collection of loggers) for experiment tracking.\n",
      " |      \n",
      " |          log_gpu_memory: None, 'min_max', 'all'. Might slow performance\n",
      " |      \n",
      " |          log_every_n_steps: How often to log within steps (defaults to every 50 steps).\n",
      " |      \n",
      " |          automatic_optimization: If False you are responsible for calling .backward, .step, zero_grad\n",
      " |              in LightningModule. This argument has been moved to LightningModule. It is deprecated\n",
      " |              here in v1.1 and will be removed in v1.3.\n",
      " |      \n",
      " |          prepare_data_per_node: If True, each LOCAL_RANK=0 will call prepare data.\n",
      " |              Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data\n",
      " |      \n",
      " |          process_position: orders the progress bar when running multiple models on same machine.\n",
      " |      \n",
      " |          progress_bar_refresh_rate: How often to refresh progress bar (in steps). Value ``0`` disables progress bar.\n",
      " |              Ignored when a custom progress bar is passed to :paramref:`~Trainer.callbacks`. Default: None, means\n",
      " |              a suitable value will be chosen based on the environment (terminal, Google COLAB, etc.).\n",
      " |      \n",
      " |          profiler: To profile individual steps during training and assist in identifying bottlenecks. Passing bool\n",
      " |              value is deprecated in v1.1 and will be removed in v1.3.\n",
      " |      \n",
      " |          overfit_batches: Overfit a percent of training data (float) or a set number of batches (int). Default: 0.0\n",
      " |      \n",
      " |          plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
      " |      \n",
      " |          precision: Full precision (32), half precision (16). Can be used on CPU, GPU or TPUs.\n",
      " |      \n",
      " |          max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
      " |              If both max_epochs and max_steps are not specified, defaults to ``max_epochs`` = 1000.\n",
      " |      \n",
      " |          min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
      " |              If both min_epochs and min_steps are not specified, defaults to ``min_epochs`` = 1.\n",
      " |      \n",
      " |          max_steps: Stop training after this number of steps. Disabled by default (None).\n",
      " |      \n",
      " |          min_steps: Force training for at least these number of steps. Disabled by default (None).\n",
      " |      \n",
      " |          num_nodes: number of GPU nodes for distributed training.\n",
      " |      \n",
      " |          num_processes: number of processes for distributed training with distributed_backend=\"ddp_cpu\"\n",
      " |      \n",
      " |          num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
      " |              Set it to `-1` to run all batches in all validation dataloaders. Default: 2\n",
      " |      \n",
      " |          reload_dataloaders_every_epoch: Set to True to reload dataloaders every epoch.\n",
      " |      \n",
      " |          replace_sampler_ddp: Explicitly enables or disables sampler replacement. If not specified this\n",
      " |              will toggled automatically when DDP is used. By default it will add ``shuffle=True`` for\n",
      " |              train sampler and ``shuffle=False`` for val/test sampler. If you want to customize it,\n",
      " |              you can set ``replace_sampler_ddp=False`` and add your own distributed sampler.\n",
      " |      \n",
      " |          resume_from_checkpoint: Path/URL of the checkpoint from which training is resumed. If there is\n",
      " |              no checkpoint file at the path, start from scratch. If resuming from mid-epoch checkpoint,\n",
      " |              training will start from the beginning of the next epoch.\n",
      " |      \n",
      " |          sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
      " |      \n",
      " |          terminate_on_nan: If set to True, will terminate training (by raising a `ValueError`) at the\n",
      " |              end of each training batch, if any of the parameters or the loss are NaN or +/-inf.\n",
      " |      \n",
      " |          tpu_cores: How many TPU cores to train on (1 or 8) / Single TPU to train on [1]\n",
      " |      \n",
      " |          track_grad_norm: -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm.\n",
      " |      \n",
      " |          truncated_bptt_steps: Truncated back prop breaks performs backprop every k steps of much longer\n",
      " |              sequence.\n",
      " |      \n",
      " |          val_check_interval: How often to check the validation set. Use float to check within a training epoch,\n",
      " |              use int to check every n steps (batches).\n",
      " |      \n",
      " |          weights_summary: Prints a summary of the weights when training begins.\n",
      " |      \n",
      " |          weights_save_path: Where to save weights if specified. Will override default_root_dir\n",
      " |              for checkpoints only. Use this if for whatever reason you need the checkpoints\n",
      " |              stored in a different place than the logs written in `default_root_dir`.\n",
      " |              Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      " |              Defaults to `default_root_dir`.\n",
      " |      \n",
      " |          move_metrics_to_cpu: Whether to force internal logged metrics to be moved to cpu.\n",
      " |              This can save some gpu memory, but can make training slower. Use with attention.\n",
      " |      \n",
      " |          enable_pl_optimizer: If True, each optimizer will be wrapped by\n",
      " |              `pytorch_lightning.core.optimizer.LightningOptimizer`. It allows Lightning to\n",
      " |              handle AMP, TPU, accumulated_gradients, etc.\n",
      " |              .. warning:: Currently deprecated and it will be removed in v1.3\n",
      " |      \n",
      " |          multiple_trainloader_mode: How to loop over the datasets when there are multiple train loaders.\n",
      " |              In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed,\n",
      " |              and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets\n",
      " |              reload when reaching the minimum length of datasets.\n",
      " |      \n",
      " |          stochastic_weight_avg: Whether to use `Stochastic Weight Averaging (SWA)\n",
      " |              <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/>_`\n",
      " |  \n",
      " |  call_hook(self, hook_name, *args, **kwargs)\n",
      " |  \n",
      " |  call_setup_hook(self, model)\n",
      " |  \n",
      " |  dispatch(self)\n",
      " |  \n",
      " |  fit(self, model: pytorch_lightning.core.lightning.LightningModule, train_dataloader: Union[torch.utils.data.dataloader.DataLoader, NoneType] = None, val_dataloaders: Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader], NoneType] = None, datamodule: Union[pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None)\n",
      " |      Runs the full optimization routine.\n",
      " |      \n",
      " |      Args:\n",
      " |          datamodule: A instance of :class:`LightningDataModule`.\n",
      " |      \n",
      " |          model: Model to fit.\n",
      " |      \n",
      " |          train_dataloader: A Pytorch DataLoader with training samples. If the model has\n",
      " |              a predefined train_dataloader method this will be skipped.\n",
      " |      \n",
      " |          val_dataloaders: Either a single Pytorch Dataloader or a list of them, specifying validation samples.\n",
      " |              If the model has a predefined val_dataloaders method this will be skipped\n",
      " |  \n",
      " |  post_dispatch(self)\n",
      " |  \n",
      " |  pre_dispatch(self)\n",
      " |  \n",
      " |  predict(self, model: Union[pytorch_lightning.core.lightning.LightningModule, NoneType] = None, dataloaders: Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader], NoneType] = None, datamodule: Union[pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None)\n",
      " |      Separates from fit to make sure you never run on your predictions set until you want to.\n",
      " |      \n",
      " |      This will call the model forward function to compute predictions.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The model to predict on.\n",
      " |      \n",
      " |          dataloaders: Either a single\n",
      " |              Pytorch Dataloader or a list of them, specifying inference samples.\n",
      " |      \n",
      " |          datamodule: A instance of :class:`LightningDataModule`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\n",
      " |  \n",
      " |  run_evaluation(self, max_batches=None, on_epoch=False)\n",
      " |  \n",
      " |  run_predict(self)\n",
      " |  \n",
      " |  run_sanity_check(self, ref_model)\n",
      " |  \n",
      " |  run_test(self)\n",
      " |  \n",
      " |  run_train(self)\n",
      " |  \n",
      " |  setup_trainer(self, model: pytorch_lightning.core.lightning.LightningModule)\n",
      " |      Sanity check a few things before starting actual training or testing.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The model to run sanity test on.\n",
      " |  \n",
      " |  test(self, model: Union[pytorch_lightning.core.lightning.LightningModule, NoneType] = None, test_dataloaders: Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader], NoneType] = None, ckpt_path: Union[str, NoneType] = 'best', verbose: bool = True, datamodule: Union[pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None)\n",
      " |      Separates from fit to make sure you never run on your test set until you want to.\n",
      " |      \n",
      " |      Args:\n",
      " |          ckpt_path: Either ``best`` or path to the checkpoint you wish to test.\n",
      " |              If ``None``, use the weights from the last epoch to test. Default to ``best``.\n",
      " |      \n",
      " |          datamodule: A instance of :class:`LightningDataModule`.\n",
      " |      \n",
      " |          model: The model to test.\n",
      " |      \n",
      " |          test_dataloaders: Either a single\n",
      " |              Pytorch Dataloader or a list of them, specifying validation samples.\n",
      " |      \n",
      " |          verbose: If True, prints the test results\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns a list of dictionaries, one for each test dataloader containing their respective metrics.\n",
      " |  \n",
      " |  track_output_for_epoch_end(self, outputs, output)\n",
      " |  \n",
      " |  train_or_test_or_predict(self)\n",
      " |  \n",
      " |  tune(self, model: pytorch_lightning.core.lightning.LightningModule, train_dataloader: Union[torch.utils.data.dataloader.DataLoader, NoneType] = None, val_dataloaders: Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader], NoneType] = None, datamodule: Union[pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None)\n",
      " |      Runs routines to tune hyperparameters before training.\n",
      " |      \n",
      " |      Args:\n",
      " |          datamodule: A instance of :class:`LightningDataModule`.\n",
      " |      \n",
      " |          model: Model to tune.\n",
      " |      \n",
      " |          train_dataloader: A Pytorch DataLoader with training samples. If the model has\n",
      " |              a predefined train_dataloader method this will be skipped.\n",
      " |      \n",
      " |          val_dataloaders: Either a single Pytorch Dataloader or a list of them, specifying validation samples.\n",
      " |              If the model has a predefined val_dataloaders method this will be skipped\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  evaluating\n",
      " |  \n",
      " |  predicting\n",
      " |  \n",
      " |  testing\n",
      " |  \n",
      " |  training\n",
      " |  \n",
      " |  tuning\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.properties.TrainerProperties:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      # TODO: refactor this so that it can be done in LightningOptimizer\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  save_checkpoint(self, filepath, weights_only: bool = False) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pytorch_lightning.trainer.properties.TrainerProperties:\n",
      " |  \n",
      " |  add_argparse_args(parent_parser: argparse.ArgumentParser) -> argparse.ArgumentParser from abc.ABCMeta\n",
      " |  \n",
      " |  default_attributes() -> dict from abc.ABCMeta\n",
      " |  \n",
      " |  from_argparse_args(args: Union[argparse.Namespace, argparse.ArgumentParser], **kwargs) -> '_T' from abc.ABCMeta\n",
      " |  \n",
      " |  get_deprecated_arg_names() -> List from abc.ABCMeta\n",
      " |      Returns a list with deprecated Trainer arguments.\n",
      " |  \n",
      " |  match_env_arguments() -> argparse.Namespace from abc.ABCMeta\n",
      " |  \n",
      " |  parse_argparser(arg_parser: Union[argparse.ArgumentParser, argparse.Namespace]) -> argparse.Namespace from abc.ABCMeta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pytorch_lightning.trainer.properties.TrainerProperties:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  accelerator\n",
      " |  \n",
      " |  amp_backend\n",
      " |  \n",
      " |  callback_metrics\n",
      " |  \n",
      " |  checkpoint_callback\n",
      " |      The first :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint`\n",
      " |      callback in the Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      " |  \n",
      " |  checkpoint_callbacks\n",
      " |      A list of all instances of :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint`\n",
      " |      found in the Trainer.callbacks list.\n",
      " |  \n",
      " |  data_parallel\n",
      " |  \n",
      " |  data_parallel_device_ids\n",
      " |  \n",
      " |  default_root_dir\n",
      " |      The default location to save artifacts of loggers, checkpoints etc.\n",
      " |      It is used as a fallback if logger or checkpoint callback do not define specific save paths.\n",
      " |  \n",
      " |  disable_validation\n",
      " |      Check if validation is disabled during training.\n",
      " |  \n",
      " |  distributed_backend\n",
      " |  \n",
      " |  distributed_sampler_kwargs\n",
      " |  \n",
      " |  early_stopping_callback\n",
      " |      The first :class:`~pytorch_lightning.callbacks.early_stopping.EarlyStopping`\n",
      " |      callback in the Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      " |  \n",
      " |  early_stopping_callbacks\n",
      " |      A list of all instances of :class:`~pytorch_lightning.callbacks.early_stopping.EarlyStopping`\n",
      " |      found in the Trainer.callbacks list.\n",
      " |  \n",
      " |  enable_validation\n",
      " |      Check if we should run validation during training.\n",
      " |  \n",
      " |  global_rank\n",
      " |  \n",
      " |  gpus\n",
      " |  \n",
      " |  is_global_zero\n",
      " |  \n",
      " |  lightning_module\n",
      " |  \n",
      " |  lightning_optimizers\n",
      " |  \n",
      " |  local_rank\n",
      " |  \n",
      " |  log_dir\n",
      " |  \n",
      " |  logged_metrics\n",
      " |  \n",
      " |  lr_schedulers\n",
      " |  \n",
      " |  model\n",
      " |      The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.\n",
      " |      To access the pure LightningModule, use\n",
      " |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.lightning_module` instead.\n",
      " |  \n",
      " |  node_rank\n",
      " |  \n",
      " |  num_gpus\n",
      " |  \n",
      " |  num_nodes\n",
      " |  \n",
      " |  num_processes\n",
      " |  \n",
      " |  optimizer_frequencies\n",
      " |  \n",
      " |  optimizers\n",
      " |  \n",
      " |  precision\n",
      " |  \n",
      " |  precision_plugin\n",
      " |  \n",
      " |  progress_bar_callback\n",
      " |  \n",
      " |  progress_bar_dict\n",
      " |      Read-only for progress bar metrics.\n",
      " |  \n",
      " |  progress_bar_metrics\n",
      " |  \n",
      " |  root_gpu\n",
      " |  \n",
      " |  scaler\n",
      " |  \n",
      " |  slurm_job_id\n",
      " |  \n",
      " |  state\n",
      " |  \n",
      " |  tpu_cores\n",
      " |  \n",
      " |  training_type_plugin\n",
      " |  \n",
      " |  use_amp\n",
      " |  \n",
      " |  weights_save_path\n",
      " |      The default root location to save weights (checkpoints), e.g., when the\n",
      " |      :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` does not define a file path.\n",
      " |  \n",
      " |  world_size\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pytorch_lightning.trainer.properties.TrainerProperties:\n",
      " |  \n",
      " |  __annotations__ = {'_default_root_dir': <class 'str'>, '_progress_bar_...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin:\n",
      " |  \n",
      " |  on_after_backward(self)\n",
      " |      Called after loss.backward() and before optimizers do anything.\n",
      " |  \n",
      " |  on_batch_end(self)\n",
      " |      Called when the training batch ends.\n",
      " |  \n",
      " |  on_batch_start(self)\n",
      " |      Called when the training batch begins.\n",
      " |  \n",
      " |  on_before_accelerator_backend_setup(self, model)\n",
      " |      Called in the beginning of fit and test\n",
      " |  \n",
      " |  on_before_zero_grad(self, optimizer)\n",
      " |      Called after optimizer.step() and before optimizer.zero_grad().\n",
      " |  \n",
      " |  on_epoch_end(self)\n",
      " |      Called when the epoch ends.\n",
      " |  \n",
      " |  on_epoch_start(self)\n",
      " |      Called when the epoch begins.\n",
      " |  \n",
      " |  on_fit_end(self)\n",
      " |      Called when the trainer initialization begins, model has not yet been set.\n",
      " |  \n",
      " |  on_fit_start(self)\n",
      " |      Called when the trainer initialization begins, model has not yet been set.\n",
      " |  \n",
      " |  on_init_end(self)\n",
      " |      Called when the trainer initialization ends, model has not yet been set.\n",
      " |  \n",
      " |  on_init_start(self)\n",
      " |      Called when the trainer initialization begins, model has not yet been set.\n",
      " |  \n",
      " |  on_keyboard_interrupt(self)\n",
      " |      Called when the training is interrupted by KeyboardInterrupt.\n",
      " |  \n",
      " |  on_load_checkpoint(self, checkpoint)\n",
      " |      Called when loading a model checkpoint.\n",
      " |  \n",
      " |  on_pretrain_routine_end(self, model)\n",
      " |      Called when the train ends.\n",
      " |  \n",
      " |  on_pretrain_routine_start(self, model)\n",
      " |      Called when the train begins.\n",
      " |  \n",
      " |  on_sanity_check_end(self)\n",
      " |      Called when the validation sanity check ends.\n",
      " |  \n",
      " |  on_sanity_check_start(self)\n",
      " |      Called when the validation sanity check starts.\n",
      " |  \n",
      " |  on_save_checkpoint(self)\n",
      " |      Called when saving a model checkpoint.\n",
      " |  \n",
      " |  on_test_batch_end(self, outputs, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the test batch ends.\n",
      " |  \n",
      " |  on_test_batch_start(self, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the test batch begins.\n",
      " |  \n",
      " |  on_test_end(self)\n",
      " |      Called when the test ends.\n",
      " |  \n",
      " |  on_test_epoch_end(self)\n",
      " |      Called when the epoch ends.\n",
      " |  \n",
      " |  on_test_epoch_start(self)\n",
      " |      Called when the epoch begins.\n",
      " |  \n",
      " |  on_test_start(self)\n",
      " |      Called when the test begins.\n",
      " |  \n",
      " |  on_train_batch_end(self, outputs, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the training batch ends.\n",
      " |  \n",
      " |  on_train_batch_start(self, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the training batch begins.\n",
      " |  \n",
      " |  on_train_end(self)\n",
      " |      Called when the train ends.\n",
      " |  \n",
      " |  on_train_epoch_end(self, outputs)\n",
      " |      Called when the epoch ends.\n",
      " |  \n",
      " |  on_train_epoch_start(self)\n",
      " |      Called when the epoch begins.\n",
      " |  \n",
      " |  on_train_start(self)\n",
      " |      Called when the train begins.\n",
      " |  \n",
      " |  on_validation_batch_end(self, outputs, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the validation batch ends.\n",
      " |  \n",
      " |  on_validation_batch_start(self, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the validation batch begins.\n",
      " |  \n",
      " |  on_validation_end(self)\n",
      " |      Called when the validation loop ends.\n",
      " |  \n",
      " |  on_validation_epoch_end(self)\n",
      " |      Called when the epoch ends.\n",
      " |  \n",
      " |  on_validation_epoch_start(self)\n",
      " |      Called when the epoch begins.\n",
      " |  \n",
      " |  on_validation_start(self)\n",
      " |      Called when the validation loop begins.\n",
      " |  \n",
      " |  setup(self, model, stage: str)\n",
      " |      Called in the beginning of fit and test\n",
      " |  \n",
      " |  teardown(self, stage: str)\n",
      " |      Called at the end of fit and test\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin:\n",
      " |  \n",
      " |  callbacks = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.model_hooks.TrainerModelHooksMixin:\n",
      " |  \n",
      " |  has_arg(self, f_name, arg_name)\n",
      " |  \n",
      " |  is_function_implemented(self, f_name, model=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.optimizers.TrainerOptimizersMixin:\n",
      " |  \n",
      " |  configure_schedulers(self, schedulers: list, monitor: Union[str, NoneType] = None)\n",
      " |  \n",
      " |  convert_to_lightning_optimizers(self)\n",
      " |  \n",
      " |  init_optimizers(self, model: pytorch_lightning.core.lightning.LightningModule) -> Tuple[List, List, List]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.logging.TrainerLoggingMixin:\n",
      " |  \n",
      " |  metrics_to_scalars(self, metrics)\n",
      " |  \n",
      " |  process_dict_result(self, output, train=False)\n",
      " |      Reduces output according to the training mode.\n",
      " |      \n",
      " |      Separates loss from logging and progress bar metrics\n",
      " |  \n",
      " |  reduce_distributed_output(self, output, num_gpus)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.training_tricks.TrainerTrainingTricksMixin:\n",
      " |  \n",
      " |  detect_nan_tensors(self, loss: torch.Tensor) -> None\n",
      " |  \n",
      " |  print_nan_gradients(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin:\n",
      " |  \n",
      " |  auto_add_sampler(self, dataloader: torch.utils.data.dataloader.DataLoader, shuffle: bool) -> torch.utils.data.dataloader.DataLoader\n",
      " |  \n",
      " |  replace_sampler(self, dataloader, sampler)\n",
      " |  \n",
      " |  request_dataloader(self, dataloader_fx: Callable) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Handles downloading data in the GPU or TPU case.\n",
      " |      \n",
      " |      Args:\n",
      " |          dataloader_fx: The bound dataloader getter\n",
      " |      \n",
      " |      Returns:\n",
      " |          The dataloader\n",
      " |  \n",
      " |  reset_predict_dataloader(self, model) -> None\n",
      " |      Resets the predict dataloader and determines the number of batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The current `LightningModule`\n",
      " |  \n",
      " |  reset_test_dataloader(self, model) -> None\n",
      " |      Resets the test dataloader and determines the number of batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The current `LightningModule`\n",
      " |  \n",
      " |  reset_train_dataloader(self, model: pytorch_lightning.core.lightning.LightningModule) -> None\n",
      " |      Resets the train dataloader and initialises required variables\n",
      " |      (number of batches, when to validate, etc.).\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The current `LightningModule`\n",
      " |  \n",
      " |  reset_val_dataloader(self, model: pytorch_lightning.core.lightning.LightningModule) -> None\n",
      " |      Resets the validation dataloader and determines the number of batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The current `LightningModule`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pytorch_lightning.trainer.deprecated_api.DeprecatedDistDeviceAttributes:\n",
      " |  \n",
      " |  on_cpu\n",
      " |  \n",
      " |  on_gpu\n",
      " |  \n",
      " |  on_tpu\n",
      " |  \n",
      " |  use_ddp\n",
      " |  \n",
      " |  use_ddp2\n",
      " |  \n",
      " |  use_dp\n",
      " |  \n",
      " |  use_horovod\n",
      " |  \n",
      " |  use_single_gpu\n",
      " |  \n",
      " |  use_tpu\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.deprecated_api.DeprecatedTrainerAttributes:\n",
      " |  \n",
      " |  get_model(self) -> pytorch_lightning.core.lightning.LightningModule\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pytorch_lightning.trainer.deprecated_api.DeprecatedTrainerAttributes:\n",
      " |  \n",
      " |  accelerator_backend\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "help(pl.Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0002910717118066608\n"
     ]
    }
   ],
   "source": [
    "fig = lr_finder.plot(True,True)\n",
    "print(lr_finder.suggestion())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lr = lr_finder.suggestion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type       | Params\n",
      "-------------------------------------------------\n",
      "0 | act               | SiLU       | 0     \n",
      "1 | x_encoder         | Linear     | 480   \n",
      "2 | edge_attr_encoder | Linear     | 112   \n",
      "3 | u_encoder         | Linear     | 1.9 K \n",
      "4 | ops               | ModuleList | 399 K \n",
      "5 | decoders          | ModuleList | 75.4 K\n",
      "6 | decoder           | Linear     | 99    \n",
      "-------------------------------------------------\n",
      "477 K     Trainable params\n",
      "0         Non-trainable params\n",
      "477 K     Total params\n",
      "1.908     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im here now!\n",
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6748c3ed654a01866179ba871d649b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 1231: Val Acc reached -0.04927 (best -0.04927), saving model to \"C:\\Users\\jv97\\Desktop\\github\\Neutrino-Machine-Learning\\checkpoints\\Test run - delete me-epoch=4-Val Acc=-0.049.ckpt\" as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Im here now!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 5104<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\jv97\\Desktop\\github\\Neutrino-Machine-Learning\\wandb\\run-20210222_114434-1103hq90\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\jv97\\Desktop\\github\\Neutrino-Machine-Learning\\wandb\\run-20210222_114434-1103hq90\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>lr-Adam</td><td>1e-05</td></tr><tr><td>_runtime</td><td>744</td></tr><tr><td>_timestamp</td><td>1613991418</td></tr><tr><td>_step</td><td>2110</td></tr><tr><td>Train Loss</td><td>-474.84387</td></tr><tr><td>Train Acc</td><td>-0.04381</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>Val Acc2</td><td>-0.04927</td></tr><tr><td>Val Acc</td><td>-0.04927</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>lr-Adam</td><td>▁▂█▇▇▁▁</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▇▇▇▇▇████</td></tr><tr><td>_step</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▇▇▇▇▇▇█████</td></tr><tr><td>Train Loss</td><td>██████████████████████▇▆█████████▇▆▅▂▂▁▁</td></tr><tr><td>Train Acc</td><td>██▇█▇██▇▄▃▄▃▃▆▃▄▃▃▄▄▃▃▂▃▄▄▃▄▃▃▃▃▂▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▃▃▃▃▃▃▅▅▅▅▅▆▆▆▆▆▆█████████████████</td></tr><tr><td>Val Acc2</td><td>█▄▄▄▃▂▁</td></tr><tr><td>Val Acc</td><td>█▄▄▄▃▂▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">Test run - delete me</strong>: <a href=\"https://wandb.ai/vinther901/Neutrino-Machine-Learning/runs/1103hq90\" target=\"_blank\">https://wandb.ai/vinther901/Neutrino-Machine-Learning/runs/1103hq90</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-19 13:58:07.486523: loading data..\n",
      "2021-02-19 13:58:08.846916: executing target constructor..\n",
      "2021-02-19 13:58:08.858912: executing feature constructor..\n",
      "2021-02-19 13:58:17.235923: shuffling dataset..\n",
      "2021-02-19 13:58:17.274909: defining dataloaders..\n",
      "2021-02-19 13:58:17.275909: Done!\n"
     ]
    }
   ],
   "source": [
    "filename = \"muon_100k_set12_SRT.pt\"\n",
    "dataset, train_loader, test_loader, val_loader = fc.dataset_preparator(name = filename,\n",
    "                                                                       path = path,\n",
    "                                                                       transformer = transformer,\n",
    "                                                                       tc = AzZe_target_constructor,\n",
    "                                                                       fc = fc.custom_feature_constructor,\n",
    "                                                                       shuffle = True,\n",
    "                                                                       TrTV_split = (0,1,1),\n",
    "                                                                       batch_size = args['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8a3227c99041abbfc7251fb2033d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test Acc': nan}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jv97\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:50: UserWarning: The testing_epoch_end should not return anything as of 9.1. To log, use self.log(...) or self.write(...) directly in the LightningModule\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Test Acc': nan}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model,test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
