{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import pandas as pd\n",
    "import time\n",
    "import wandb\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import FunctionCollection as fc\n",
    "import importlib\n",
    "fc = importlib.reload(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AzZe_target_constructor(dataset, transformer):\n",
    "    az_ze = dataset.data.y.view(-1,10,1)[:,[8,9]]\n",
    "    tfs = transformer\n",
    "    \n",
    "    az = torch.tensor(tfs['truth']['azimuth'].inverse_transform(az_ze[:,0]),dtype=torch.float)\n",
    "    ze = torch.tensor(tfs['truth']['zenith'].inverse_transform(az_ze[:,1]),dtype=torch.float) #the range seems to be about [0,pi/2]?. Makes sense, Muons come from the atmosphere\n",
    "    \n",
    "    dataset.data.y = torch.cat([az,ze],dim=1).flatten()\n",
    "    dataset.slices['y'] = np.arange(0,len(dataset.data.y)+1, 2)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-19 22:24:12.182999: loading data..\n",
      "2021-02-19 22:24:12.430065: executing target constructor..\n",
      "2021-02-19 22:24:12.434065: executing feature constructor..\n",
      "2021-02-19 22:24:21.501501: shuffling dataset..\n",
      "2021-02-19 22:24:21.531492: defining dataloaders..\n",
      "2021-02-19 22:24:21.532493: Done!\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/jv97/Desktop/github/Neutrino-Machine-Learning/datasets\"\n",
    "filename = \"muon_100k_set11_SRT.pt\"\n",
    "transformer = pd.read_pickle(path + \"/transformers.pkl\")\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "dataset, train_loader, test_loader, val_loader = fc.dataset_preparator(name = filename,\n",
    "                                                                       path = path,\n",
    "                                                                       transformer = transformer,\n",
    "                                                                       tc = AzZe_target_constructor,\n",
    "                                                                       fc = fc.custom_feature_constructor,\n",
    "                                                                       shuffle = True,\n",
    "                                                                       TrTV_split = (0.9,0.9,1),\n",
    "                                                                       batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remember all accuracies are positive and defined to go towards 0 in the optimal case.\n"
     ]
    }
   ],
   "source": [
    "import Model_tmp as M\n",
    "M = importlib.reload(M)\n",
    "\n",
    "args = {'N_edge_feats': 6,\n",
    "        'N_dom_feats': 5,\n",
    "        'N_targets': 3,\n",
    "        'N_metalayers': 5,\n",
    "        'N_hcs': 16,\n",
    "        'wandb_activated': True,\n",
    "        'lr': 1e-3,\n",
    "        'batch_size': batch_size}\n",
    "\n",
    "model = M.Load_model('Spherical_NLLH',args)\n",
    "\n",
    "# if args['wandb_activated']:\n",
    "#     wandb.login()\n",
    "#     wandb.init(name='Test run - Delete me',\n",
    "#               project='Neutrino-Machine-Learning',\n",
    "#               notes='')\n",
    "#     wandb.config.lr = args['lr']\n",
    "#     wandb.config.batch_size = args['batch_size']\n",
    "#     wandb.config.scheduler = 'epoch_warmup=2,max=25,a=0.05,q=0.2'\n",
    "#     wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.19<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">Test run - delete me</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/vinther901/Neutrino-Machine-Learning\" target=\"_blank\">https://wandb.ai/vinther901/Neutrino-Machine-Learning</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/vinther901/Neutrino-Machine-Learning/runs/3cozkso9\" target=\"_blank\">https://wandb.ai/vinther901/Neutrino-Machine-Learning/runs/3cozkso9</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\jv97\\Desktop\\github\\Neutrino-Machine-Learning\\wandb\\run-20210219_222504-3cozkso9</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "early_stop_callback = pl.callbacks.early_stopping.EarlyStopping(monitor='Val Acc', min_delta=0.00, patience=3, verbose=False, mode='min')\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "wandb_logger = WandbLogger(name='Test run - delete me',\n",
    "                           project='Neutrino-Machine-Learning')\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, #-1 for all gpus\n",
    "                     max_epochs=1, \n",
    "                     callbacks=[early_stop_callback], \n",
    "                     logger = wandb_logger if args['wandb_activated'] else False)\n",
    "\n",
    "if args['wandb_activated']:\n",
    "    wandb_logger.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name              | Type       | Params\n",
      "-------------------------------------------------\n",
      "0 | act               | SiLU       | 0     \n",
      "1 | x_encoder         | Linear     | 480   \n",
      "2 | edge_attr_encoder | Linear     | 112   \n",
      "3 | u_encoder         | Linear     | 1.9 K \n",
      "4 | ops               | ModuleList | 399 K \n",
      "5 | decoders          | ModuleList | 75.4 K\n",
      "6 | decoder           | Linear     | 99    \n",
      "-------------------------------------------------\n",
      "477 K     Trainable params\n",
      "0         Non-trainable params\n",
      "477 K     Total params\n",
      "1.908     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65068a04f0694a029b9d93e51bf4cd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Step must only increase in log calls.  Step 175 < 674; dropping {'val_batch_acc_step/epoch_0': 0.01317131519317627}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 176 < 674; dropping {'val_batch_acc_step/epoch_0': 0.0308382511138916}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 177 < 674; dropping {'val_batch_acc_step/epoch_0': 0.037039756774902344}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 178 < 674; dropping {'val_batch_acc_step/epoch_0': 0.0026885271072387695}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 179 < 674; dropping {'val_batch_acc_step/epoch_0': 0.01820605993270874}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 180 < 674; dropping {'val_batch_acc_step/epoch_0': 0.021343231201171875}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 181 < 674; dropping {'val_batch_acc_step/epoch_0': 0.037691354751586914}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 182 < 674; dropping {'val_batch_acc_step/epoch_0': 0.03642702102661133}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 183 < 674; dropping {'val_batch_acc_step/epoch_0': 0.01634514331817627}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 184 < 674; dropping {'val_batch_acc_step/epoch_0': 0.0404362678527832}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 185 < 674; dropping {'val_batch_acc_step/epoch_0': 0.02469867467880249}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 186 < 674; dropping {'val_batch_acc_step/epoch_0': 0.024892747402191162}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 187 < 674; dropping {'val_batch_acc_step/epoch_0': 0.016870975494384766}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 188 < 674; dropping {'val_batch_acc_step/epoch_0': 0.00769197940826416}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 189 < 674; dropping {'val_batch_acc_step/epoch_0': 0.033517420291900635}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 190 < 674; dropping {'val_batch_acc_step/epoch_0': 0.02822411060333252}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 191 < 674; dropping {'val_batch_acc_step/epoch_0': 0.0020104646682739258}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 192 < 674; dropping {'val_batch_acc_step/epoch_0': 0.02976590394973755}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 193 < 674; dropping {'val_batch_acc_step/epoch_0': 0.039919137954711914}.\n",
      "wandb: WARNING Step must only increase in log calls.  Step 194 < 674; dropping {'val_batch_acc_step/epoch_0': 0.039890408515930176}.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 3676<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.01MB of 0.01MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\jv97\\Desktop\\github\\Neutrino-Machine-Learning\\wandb\\run-20210219_222504-3cozkso9\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\jv97\\Desktop\\github\\Neutrino-Machine-Learning\\wandb\\run-20210219_222504-3cozkso9\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train Loss</td><td>-64.21049</td></tr><tr><td>Train Acc</td><td>0.03901</td></tr><tr><td>epoch</td><td>0</td></tr><tr><td>_runtime</td><td>354</td></tr><tr><td>_timestamp</td><td>1613770258</td></tr><tr><td>_step</td><td>702</td></tr><tr><td>val_batch_acc_epoch</td><td>0.02508</td></tr><tr><td>Val Acc</td><td>0.02508</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train Loss</td><td>█████▅▆▅▄▁</td></tr><tr><td>Train Acc</td><td>██▃▂▂▁▂▂▁▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_runtime</td><td>▁▂▂▃▄▄▄▄▆▆▇▇█</td></tr><tr><td>_timestamp</td><td>▁▂▂▃▄▄▄▄▆▆▇▇█</td></tr><tr><td>_step</td><td>▁▂▂▂▄▅▆▆▆▇▇██</td></tr><tr><td>val_batch_acc_epoch</td><td>█▃▁</td></tr><tr><td>Val Acc</td><td>█▃▁</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">Test run - delete me</strong>: <a href=\"https://wandb.ai/vinther901/Neutrino-Machine-Learning/runs/3cozkso9\" target=\"_blank\">https://wandb.ai/vinther901/Neutrino-Machine-Learning/runs/3cozkso9</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method log_metrics in module pytorch_lightning.loggers.wandb:\n",
      "\n",
      "log_metrics(metrics: Dict[str, float], step: Union[int, NoneType] = None) -> None method of pytorch_lightning.loggers.wandb.WandbLogger instance\n",
      "    Records metrics.\n",
      "    This method logs metrics as as soon as it received them. If you want to aggregate\n",
      "    metrics for one specific `step`, use the\n",
      "    :meth:`~pytorch_lightning.loggers.base.LightningLoggerBase.agg_and_log_metrics` method.\n",
      "    \n",
      "    Args:\n",
      "        metrics: Dictionary with metric names as keys and measured quantities as values\n",
      "        step: Step number at which the metrics should be recorded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(model.logger.log_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Trainer in module pytorch_lightning.trainer.trainer:\n",
      "\n",
      "class Trainer(pytorch_lightning.trainer.properties.TrainerProperties, pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin, pytorch_lightning.trainer.model_hooks.TrainerModelHooksMixin, pytorch_lightning.trainer.optimizers.TrainerOptimizersMixin, pytorch_lightning.trainer.logging.TrainerLoggingMixin, pytorch_lightning.trainer.training_tricks.TrainerTrainingTricksMixin, pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin, pytorch_lightning.trainer.deprecated_api.DeprecatedDistDeviceAttributes, pytorch_lightning.trainer.deprecated_api.DeprecatedTrainerAttributes)\n",
      " |  Trainer(logger: Union[pytorch_lightning.loggers.base.LightningLoggerBase, Iterable[pytorch_lightning.loggers.base.LightningLoggerBase], bool] = True, checkpoint_callback: bool = True, callbacks: Union[List[pytorch_lightning.callbacks.base.Callback], pytorch_lightning.callbacks.base.Callback, NoneType] = None, default_root_dir: Union[str, NoneType] = None, gradient_clip_val: float = 0, process_position: int = 0, num_nodes: int = 1, num_processes: int = 1, gpus: Union[int, str, List[int], NoneType] = None, auto_select_gpus: bool = False, tpu_cores: Union[int, str, List[int], NoneType] = None, log_gpu_memory: Union[str, NoneType] = None, progress_bar_refresh_rate: Union[int, NoneType] = None, overfit_batches: Union[int, float] = 0.0, track_grad_norm: Union[int, float, str] = -1, check_val_every_n_epoch: int = 1, fast_dev_run: Union[int, bool] = False, accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1, max_epochs: Union[int, NoneType] = None, min_epochs: Union[int, NoneType] = None, max_steps: Union[int, NoneType] = None, min_steps: Union[int, NoneType] = None, limit_train_batches: Union[int, float] = 1.0, limit_val_batches: Union[int, float] = 1.0, limit_test_batches: Union[int, float] = 1.0, limit_predict_batches: Union[int, float] = 1.0, val_check_interval: Union[int, float] = 1.0, flush_logs_every_n_steps: int = 100, log_every_n_steps: int = 50, accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator, NoneType] = None, sync_batchnorm: bool = False, precision: int = 32, weights_summary: Union[str, NoneType] = 'top', weights_save_path: Union[str, NoneType] = None, num_sanity_val_steps: int = 2, truncated_bptt_steps: Union[int, NoneType] = None, resume_from_checkpoint: Union[str, pathlib.Path, NoneType] = None, profiler: Union[pytorch_lightning.profiler.profilers.BaseProfiler, bool, str, NoneType] = None, benchmark: bool = False, deterministic: bool = False, reload_dataloaders_every_epoch: bool = False, auto_lr_find: Union[bool, str] = False, replace_sampler_ddp: bool = True, terminate_on_nan: bool = False, auto_scale_batch_size: Union[str, bool] = False, prepare_data_per_node: bool = True, plugins: Union[pytorch_lightning.plugins.base_plugin.Plugin, str, list, NoneType] = None, amp_backend: str = 'native', amp_level: str = 'O2', distributed_backend: Union[str, NoneType] = None, automatic_optimization: Union[bool, NoneType] = None, move_metrics_to_cpu: bool = False, enable_pl_optimizer: bool = None, multiple_trainloader_mode: str = 'max_size_cycle', stochastic_weight_avg: bool = False)\n",
      " |  \n",
      " |  Helper class that provides a standard way to create an ABC using\n",
      " |  inheritance.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Trainer\n",
      " |      pytorch_lightning.trainer.properties.TrainerProperties\n",
      " |      pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin\n",
      " |      pytorch_lightning.trainer.model_hooks.TrainerModelHooksMixin\n",
      " |      pytorch_lightning.trainer.optimizers.TrainerOptimizersMixin\n",
      " |      pytorch_lightning.trainer.logging.TrainerLoggingMixin\n",
      " |      pytorch_lightning.trainer.training_tricks.TrainerTrainingTricksMixin\n",
      " |      pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin\n",
      " |      abc.ABC\n",
      " |      pytorch_lightning.trainer.deprecated_api.DeprecatedDistDeviceAttributes\n",
      " |      pytorch_lightning.trainer.deprecated_api.DeprecatedTrainerAttributes\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, logger: Union[pytorch_lightning.loggers.base.LightningLoggerBase, Iterable[pytorch_lightning.loggers.base.LightningLoggerBase], bool] = True, checkpoint_callback: bool = True, callbacks: Union[List[pytorch_lightning.callbacks.base.Callback], pytorch_lightning.callbacks.base.Callback, NoneType] = None, default_root_dir: Union[str, NoneType] = None, gradient_clip_val: float = 0, process_position: int = 0, num_nodes: int = 1, num_processes: int = 1, gpus: Union[int, str, List[int], NoneType] = None, auto_select_gpus: bool = False, tpu_cores: Union[int, str, List[int], NoneType] = None, log_gpu_memory: Union[str, NoneType] = None, progress_bar_refresh_rate: Union[int, NoneType] = None, overfit_batches: Union[int, float] = 0.0, track_grad_norm: Union[int, float, str] = -1, check_val_every_n_epoch: int = 1, fast_dev_run: Union[int, bool] = False, accumulate_grad_batches: Union[int, Dict[int, int], List[list]] = 1, max_epochs: Union[int, NoneType] = None, min_epochs: Union[int, NoneType] = None, max_steps: Union[int, NoneType] = None, min_steps: Union[int, NoneType] = None, limit_train_batches: Union[int, float] = 1.0, limit_val_batches: Union[int, float] = 1.0, limit_test_batches: Union[int, float] = 1.0, limit_predict_batches: Union[int, float] = 1.0, val_check_interval: Union[int, float] = 1.0, flush_logs_every_n_steps: int = 100, log_every_n_steps: int = 50, accelerator: Union[str, pytorch_lightning.accelerators.accelerator.Accelerator, NoneType] = None, sync_batchnorm: bool = False, precision: int = 32, weights_summary: Union[str, NoneType] = 'top', weights_save_path: Union[str, NoneType] = None, num_sanity_val_steps: int = 2, truncated_bptt_steps: Union[int, NoneType] = None, resume_from_checkpoint: Union[str, pathlib.Path, NoneType] = None, profiler: Union[pytorch_lightning.profiler.profilers.BaseProfiler, bool, str, NoneType] = None, benchmark: bool = False, deterministic: bool = False, reload_dataloaders_every_epoch: bool = False, auto_lr_find: Union[bool, str] = False, replace_sampler_ddp: bool = True, terminate_on_nan: bool = False, auto_scale_batch_size: Union[str, bool] = False, prepare_data_per_node: bool = True, plugins: Union[pytorch_lightning.plugins.base_plugin.Plugin, str, list, NoneType] = None, amp_backend: str = 'native', amp_level: str = 'O2', distributed_backend: Union[str, NoneType] = None, automatic_optimization: Union[bool, NoneType] = None, move_metrics_to_cpu: bool = False, enable_pl_optimizer: bool = None, multiple_trainloader_mode: str = 'max_size_cycle', stochastic_weight_avg: bool = False)\n",
      " |      Customize every aspect of training via flags\n",
      " |      \n",
      " |      Args:\n",
      " |      \n",
      " |          accelerator: Previously known as distributed_backend (dp, ddp, ddp2, etc...).\n",
      " |              Can also take in an accelerator object for custom hardware.\n",
      " |      \n",
      " |          accumulate_grad_batches: Accumulates grads every k batches or as set up in the dict.\n",
      " |      \n",
      " |          amp_backend: The mixed precision backend to use (\"native\" or \"apex\")\n",
      " |      \n",
      " |          amp_level: The optimization level to use (O1, O2, etc...).\n",
      " |      \n",
      " |          auto_lr_find: If set to True, will make trainer.tune() run a learning rate finder,\n",
      " |              trying to optimize initial learning for faster convergence. trainer.tune() method will\n",
      " |              set the suggested learning rate in self.lr or self.learning_rate in the LightningModule.\n",
      " |              To use a different key set a string instead of True with the key name.\n",
      " |      \n",
      " |          auto_scale_batch_size: If set to True, will `initially` run a batch size\n",
      " |              finder trying to find the largest batch size that fits into memory.\n",
      " |              The result will be stored in self.batch_size in the LightningModule.\n",
      " |              Additionally, can be set to either `power` that estimates the batch size through\n",
      " |              a power search or `binsearch` that estimates the batch size through a binary search.\n",
      " |      \n",
      " |          auto_select_gpus: If enabled and `gpus` is an integer, pick available\n",
      " |              gpus automatically. This is especially useful when\n",
      " |              GPUs are configured to be in \"exclusive mode\", such\n",
      " |              that only one process at a time can access them.\n",
      " |      \n",
      " |          benchmark: If true enables cudnn.benchmark.\n",
      " |      \n",
      " |          callbacks: Add a callback or list of callbacks.\n",
      " |      \n",
      " |          checkpoint_callback: If ``True``, enable checkpointing.\n",
      " |              It will configure a default ModelCheckpoint callback if there is no user-defined ModelCheckpoint in\n",
      " |              :paramref:`~pytorch_lightning.trainer.trainer.Trainer.callbacks`. Default: ``True``.\n",
      " |      \n",
      " |              .. warning:: Passing a ModelCheckpoint instance to this argument is deprecated since\n",
      " |                  v1.1 and will be unsupported from v1.3. Use `callbacks` argument instead.\n",
      " |      \n",
      " |          check_val_every_n_epoch: Check val every n train epochs.\n",
      " |      \n",
      " |          default_root_dir: Default path for logs and weights when no logger/ckpt_callback passed.\n",
      " |              Default: ``os.getcwd()``.\n",
      " |              Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      " |      \n",
      " |          deterministic: If true enables cudnn.deterministic.\n",
      " |      \n",
      " |          distributed_backend: deprecated. Please use 'accelerator'\n",
      " |      \n",
      " |          fast_dev_run: runs n if set to ``n`` (int) else 1 if set to ``True`` batch(es)\n",
      " |              of train, val and test to find any bugs (ie: a sort of unit test).\n",
      " |      \n",
      " |          flush_logs_every_n_steps: How often to flush logs to disk (defaults to every 100 steps).\n",
      " |      \n",
      " |          gpus: number of gpus to train on (int) or which GPUs to train on (list or str) applied per node\n",
      " |      \n",
      " |          gradient_clip_val: 0 means don't clip.\n",
      " |      \n",
      " |          limit_train_batches: How much of training dataset to check (floats = percent, int = num_batches)\n",
      " |      \n",
      " |          limit_val_batches: How much of validation dataset to check (floats = percent, int = num_batches)\n",
      " |      \n",
      " |          limit_test_batches: How much of test dataset to check (floats = percent, int = num_batches)\n",
      " |      \n",
      " |          logger: Logger (or iterable collection of loggers) for experiment tracking.\n",
      " |      \n",
      " |          log_gpu_memory: None, 'min_max', 'all'. Might slow performance\n",
      " |      \n",
      " |          log_every_n_steps: How often to log within steps (defaults to every 50 steps).\n",
      " |      \n",
      " |          automatic_optimization: If False you are responsible for calling .backward, .step, zero_grad\n",
      " |              in LightningModule. This argument has been moved to LightningModule. It is deprecated\n",
      " |              here in v1.1 and will be removed in v1.3.\n",
      " |      \n",
      " |          prepare_data_per_node: If True, each LOCAL_RANK=0 will call prepare data.\n",
      " |              Otherwise only NODE_RANK=0, LOCAL_RANK=0 will prepare data\n",
      " |      \n",
      " |          process_position: orders the progress bar when running multiple models on same machine.\n",
      " |      \n",
      " |          progress_bar_refresh_rate: How often to refresh progress bar (in steps). Value ``0`` disables progress bar.\n",
      " |              Ignored when a custom progress bar is passed to :paramref:`~Trainer.callbacks`. Default: None, means\n",
      " |              a suitable value will be chosen based on the environment (terminal, Google COLAB, etc.).\n",
      " |      \n",
      " |          profiler: To profile individual steps during training and assist in identifying bottlenecks. Passing bool\n",
      " |              value is deprecated in v1.1 and will be removed in v1.3.\n",
      " |      \n",
      " |          overfit_batches: Overfit a percent of training data (float) or a set number of batches (int). Default: 0.0\n",
      " |      \n",
      " |          plugins: Plugins allow modification of core behavior like ddp and amp, and enable custom lightning plugins.\n",
      " |      \n",
      " |          precision: Full precision (32), half precision (16). Can be used on CPU, GPU or TPUs.\n",
      " |      \n",
      " |          max_epochs: Stop training once this number of epochs is reached. Disabled by default (None).\n",
      " |              If both max_epochs and max_steps are not specified, defaults to ``max_epochs`` = 1000.\n",
      " |      \n",
      " |          min_epochs: Force training for at least these many epochs. Disabled by default (None).\n",
      " |              If both min_epochs and min_steps are not specified, defaults to ``min_epochs`` = 1.\n",
      " |      \n",
      " |          max_steps: Stop training after this number of steps. Disabled by default (None).\n",
      " |      \n",
      " |          min_steps: Force training for at least these number of steps. Disabled by default (None).\n",
      " |      \n",
      " |          num_nodes: number of GPU nodes for distributed training.\n",
      " |      \n",
      " |          num_processes: number of processes for distributed training with distributed_backend=\"ddp_cpu\"\n",
      " |      \n",
      " |          num_sanity_val_steps: Sanity check runs n validation batches before starting the training routine.\n",
      " |              Set it to `-1` to run all batches in all validation dataloaders. Default: 2\n",
      " |      \n",
      " |          reload_dataloaders_every_epoch: Set to True to reload dataloaders every epoch.\n",
      " |      \n",
      " |          replace_sampler_ddp: Explicitly enables or disables sampler replacement. If not specified this\n",
      " |              will toggled automatically when DDP is used. By default it will add ``shuffle=True`` for\n",
      " |              train sampler and ``shuffle=False`` for val/test sampler. If you want to customize it,\n",
      " |              you can set ``replace_sampler_ddp=False`` and add your own distributed sampler.\n",
      " |      \n",
      " |          resume_from_checkpoint: Path/URL of the checkpoint from which training is resumed. If there is\n",
      " |              no checkpoint file at the path, start from scratch. If resuming from mid-epoch checkpoint,\n",
      " |              training will start from the beginning of the next epoch.\n",
      " |      \n",
      " |          sync_batchnorm: Synchronize batch norm layers between process groups/whole world.\n",
      " |      \n",
      " |          terminate_on_nan: If set to True, will terminate training (by raising a `ValueError`) at the\n",
      " |              end of each training batch, if any of the parameters or the loss are NaN or +/-inf.\n",
      " |      \n",
      " |          tpu_cores: How many TPU cores to train on (1 or 8) / Single TPU to train on [1]\n",
      " |      \n",
      " |          track_grad_norm: -1 no tracking. Otherwise tracks that p-norm. May be set to 'inf' infinity-norm.\n",
      " |      \n",
      " |          truncated_bptt_steps: Truncated back prop breaks performs backprop every k steps of much longer\n",
      " |              sequence.\n",
      " |      \n",
      " |          val_check_interval: How often to check the validation set. Use float to check within a training epoch,\n",
      " |              use int to check every n steps (batches).\n",
      " |      \n",
      " |          weights_summary: Prints a summary of the weights when training begins.\n",
      " |      \n",
      " |          weights_save_path: Where to save weights if specified. Will override default_root_dir\n",
      " |              for checkpoints only. Use this if for whatever reason you need the checkpoints\n",
      " |              stored in a different place than the logs written in `default_root_dir`.\n",
      " |              Can be remote file paths such as `s3://mybucket/path` or 'hdfs://path/'\n",
      " |              Defaults to `default_root_dir`.\n",
      " |      \n",
      " |          move_metrics_to_cpu: Whether to force internal logged metrics to be moved to cpu.\n",
      " |              This can save some gpu memory, but can make training slower. Use with attention.\n",
      " |      \n",
      " |          enable_pl_optimizer: If True, each optimizer will be wrapped by\n",
      " |              `pytorch_lightning.core.optimizer.LightningOptimizer`. It allows Lightning to\n",
      " |              handle AMP, TPU, accumulated_gradients, etc.\n",
      " |              .. warning:: Currently deprecated and it will be removed in v1.3\n",
      " |      \n",
      " |          multiple_trainloader_mode: How to loop over the datasets when there are multiple train loaders.\n",
      " |              In 'max_size_cycle' mode, the trainer ends one epoch when the largest dataset is traversed,\n",
      " |              and smaller datasets reload when running out of their data. In 'min_size' mode, all the datasets\n",
      " |              reload when reaching the minimum length of datasets.\n",
      " |      \n",
      " |          stochastic_weight_avg: Whether to use `Stochastic Weight Averaging (SWA)\n",
      " |              <https://pytorch.org/blog/pytorch-1.6-now-includes-stochastic-weight-averaging/>_`\n",
      " |  \n",
      " |  call_hook(self, hook_name, *args, **kwargs)\n",
      " |  \n",
      " |  call_setup_hook(self, model)\n",
      " |  \n",
      " |  dispatch(self)\n",
      " |  \n",
      " |  fit(self, model: pytorch_lightning.core.lightning.LightningModule, train_dataloader: Union[torch.utils.data.dataloader.DataLoader, NoneType] = None, val_dataloaders: Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader], NoneType] = None, datamodule: Union[pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None)\n",
      " |      Runs the full optimization routine.\n",
      " |      \n",
      " |      Args:\n",
      " |          datamodule: A instance of :class:`LightningDataModule`.\n",
      " |      \n",
      " |          model: Model to fit.\n",
      " |      \n",
      " |          train_dataloader: A Pytorch DataLoader with training samples. If the model has\n",
      " |              a predefined train_dataloader method this will be skipped.\n",
      " |      \n",
      " |          val_dataloaders: Either a single Pytorch Dataloader or a list of them, specifying validation samples.\n",
      " |              If the model has a predefined val_dataloaders method this will be skipped\n",
      " |  \n",
      " |  post_dispatch(self)\n",
      " |  \n",
      " |  pre_dispatch(self)\n",
      " |  \n",
      " |  predict(self, model: Union[pytorch_lightning.core.lightning.LightningModule, NoneType] = None, dataloaders: Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader], NoneType] = None, datamodule: Union[pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None)\n",
      " |      Separates from fit to make sure you never run on your predictions set until you want to.\n",
      " |      \n",
      " |      This will call the model forward function to compute predictions.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The model to predict on.\n",
      " |      \n",
      " |          dataloaders: Either a single\n",
      " |              Pytorch Dataloader or a list of them, specifying inference samples.\n",
      " |      \n",
      " |          datamodule: A instance of :class:`LightningDataModule`.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns a list of dictionaries, one for each provided dataloader containing their respective predictions.\n",
      " |  \n",
      " |  run_evaluation(self, max_batches=None, on_epoch=False)\n",
      " |  \n",
      " |  run_predict(self)\n",
      " |  \n",
      " |  run_sanity_check(self, ref_model)\n",
      " |  \n",
      " |  run_test(self)\n",
      " |  \n",
      " |  run_train(self)\n",
      " |  \n",
      " |  setup_trainer(self, model: pytorch_lightning.core.lightning.LightningModule)\n",
      " |      Sanity check a few things before starting actual training or testing.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The model to run sanity test on.\n",
      " |  \n",
      " |  test(self, model: Union[pytorch_lightning.core.lightning.LightningModule, NoneType] = None, test_dataloaders: Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader], NoneType] = None, ckpt_path: Union[str, NoneType] = 'best', verbose: bool = True, datamodule: Union[pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None)\n",
      " |      Separates from fit to make sure you never run on your test set until you want to.\n",
      " |      \n",
      " |      Args:\n",
      " |          ckpt_path: Either ``best`` or path to the checkpoint you wish to test.\n",
      " |              If ``None``, use the weights from the last epoch to test. Default to ``best``.\n",
      " |      \n",
      " |          datamodule: A instance of :class:`LightningDataModule`.\n",
      " |      \n",
      " |          model: The model to test.\n",
      " |      \n",
      " |          test_dataloaders: Either a single\n",
      " |              Pytorch Dataloader or a list of them, specifying validation samples.\n",
      " |      \n",
      " |          verbose: If True, prints the test results\n",
      " |      \n",
      " |      Returns:\n",
      " |          Returns a list of dictionaries, one for each test dataloader containing their respective metrics.\n",
      " |  \n",
      " |  track_output_for_epoch_end(self, outputs, output)\n",
      " |  \n",
      " |  train_or_test_or_predict(self)\n",
      " |  \n",
      " |  tune(self, model: pytorch_lightning.core.lightning.LightningModule, train_dataloader: Union[torch.utils.data.dataloader.DataLoader, NoneType] = None, val_dataloaders: Union[torch.utils.data.dataloader.DataLoader, List[torch.utils.data.dataloader.DataLoader], NoneType] = None, datamodule: Union[pytorch_lightning.core.datamodule.LightningDataModule, NoneType] = None)\n",
      " |      Runs routines to tune hyperparameters before training.\n",
      " |      \n",
      " |      Args:\n",
      " |          datamodule: A instance of :class:`LightningDataModule`.\n",
      " |      \n",
      " |          model: Model to tune.\n",
      " |      \n",
      " |          train_dataloader: A Pytorch DataLoader with training samples. If the model has\n",
      " |              a predefined train_dataloader method this will be skipped.\n",
      " |      \n",
      " |          val_dataloaders: Either a single Pytorch Dataloader or a list of them, specifying validation samples.\n",
      " |              If the model has a predefined val_dataloaders method this will be skipped\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  evaluating\n",
      " |  \n",
      " |  predicting\n",
      " |  \n",
      " |  testing\n",
      " |  \n",
      " |  training\n",
      " |  \n",
      " |  tuning\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.properties.TrainerProperties:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |      # TODO: refactor this so that it can be done in LightningOptimizer\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  save_checkpoint(self, filepath, weights_only: bool = False) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from pytorch_lightning.trainer.properties.TrainerProperties:\n",
      " |  \n",
      " |  add_argparse_args(parent_parser: argparse.ArgumentParser) -> argparse.ArgumentParser from abc.ABCMeta\n",
      " |  \n",
      " |  default_attributes() -> dict from abc.ABCMeta\n",
      " |  \n",
      " |  from_argparse_args(args: Union[argparse.Namespace, argparse.ArgumentParser], **kwargs) -> '_T' from abc.ABCMeta\n",
      " |  \n",
      " |  get_deprecated_arg_names() -> List from abc.ABCMeta\n",
      " |      Returns a list with deprecated Trainer arguments.\n",
      " |  \n",
      " |  match_env_arguments() -> argparse.Namespace from abc.ABCMeta\n",
      " |  \n",
      " |  parse_argparser(arg_parser: Union[argparse.ArgumentParser, argparse.Namespace]) -> argparse.Namespace from abc.ABCMeta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pytorch_lightning.trainer.properties.TrainerProperties:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  accelerator\n",
      " |  \n",
      " |  amp_backend\n",
      " |  \n",
      " |  callback_metrics\n",
      " |  \n",
      " |  checkpoint_callback\n",
      " |      The first :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint`\n",
      " |      callback in the Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      " |  \n",
      " |  checkpoint_callbacks\n",
      " |      A list of all instances of :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint`\n",
      " |      found in the Trainer.callbacks list.\n",
      " |  \n",
      " |  data_parallel\n",
      " |  \n",
      " |  data_parallel_device_ids\n",
      " |  \n",
      " |  default_root_dir\n",
      " |      The default location to save artifacts of loggers, checkpoints etc.\n",
      " |      It is used as a fallback if logger or checkpoint callback do not define specific save paths.\n",
      " |  \n",
      " |  disable_validation\n",
      " |      Check if validation is disabled during training.\n",
      " |  \n",
      " |  distributed_backend\n",
      " |  \n",
      " |  distributed_sampler_kwargs\n",
      " |  \n",
      " |  early_stopping_callback\n",
      " |      The first :class:`~pytorch_lightning.callbacks.early_stopping.EarlyStopping`\n",
      " |      callback in the Trainer.callbacks list, or ``None`` if it doesn't exist.\n",
      " |  \n",
      " |  early_stopping_callbacks\n",
      " |      A list of all instances of :class:`~pytorch_lightning.callbacks.early_stopping.EarlyStopping`\n",
      " |      found in the Trainer.callbacks list.\n",
      " |  \n",
      " |  enable_validation\n",
      " |      Check if we should run validation during training.\n",
      " |  \n",
      " |  global_rank\n",
      " |  \n",
      " |  gpus\n",
      " |  \n",
      " |  is_global_zero\n",
      " |  \n",
      " |  lightning_module\n",
      " |  \n",
      " |  lightning_optimizers\n",
      " |  \n",
      " |  local_rank\n",
      " |  \n",
      " |  log_dir\n",
      " |  \n",
      " |  logged_metrics\n",
      " |  \n",
      " |  lr_schedulers\n",
      " |  \n",
      " |  model\n",
      " |      The LightningModule, but possibly wrapped into DataParallel or DistributedDataParallel.\n",
      " |      To access the pure LightningModule, use\n",
      " |      :meth:`~pytorch_lightning.trainer.trainer.Trainer.lightning_module` instead.\n",
      " |  \n",
      " |  node_rank\n",
      " |  \n",
      " |  num_gpus\n",
      " |  \n",
      " |  num_nodes\n",
      " |  \n",
      " |  num_processes\n",
      " |  \n",
      " |  optimizer_frequencies\n",
      " |  \n",
      " |  optimizers\n",
      " |  \n",
      " |  precision\n",
      " |  \n",
      " |  precision_plugin\n",
      " |  \n",
      " |  progress_bar_callback\n",
      " |  \n",
      " |  progress_bar_dict\n",
      " |      Read-only for progress bar metrics.\n",
      " |  \n",
      " |  progress_bar_metrics\n",
      " |  \n",
      " |  root_gpu\n",
      " |  \n",
      " |  scaler\n",
      " |  \n",
      " |  slurm_job_id\n",
      " |  \n",
      " |  state\n",
      " |  \n",
      " |  tpu_cores\n",
      " |  \n",
      " |  training_type_plugin\n",
      " |  \n",
      " |  use_amp\n",
      " |  \n",
      " |  weights_save_path\n",
      " |      The default root location to save weights (checkpoints), e.g., when the\n",
      " |      :class:`~pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint` does not define a file path.\n",
      " |  \n",
      " |  world_size\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pytorch_lightning.trainer.properties.TrainerProperties:\n",
      " |  \n",
      " |  __annotations__ = {'_default_root_dir': <class 'str'>, '_progress_bar_...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin:\n",
      " |  \n",
      " |  on_after_backward(self)\n",
      " |      Called after loss.backward() and before optimizers do anything.\n",
      " |  \n",
      " |  on_batch_end(self)\n",
      " |      Called when the training batch ends.\n",
      " |  \n",
      " |  on_batch_start(self)\n",
      " |      Called when the training batch begins.\n",
      " |  \n",
      " |  on_before_accelerator_backend_setup(self, model)\n",
      " |      Called in the beginning of fit and test\n",
      " |  \n",
      " |  on_before_zero_grad(self, optimizer)\n",
      " |      Called after optimizer.step() and before optimizer.zero_grad().\n",
      " |  \n",
      " |  on_epoch_end(self)\n",
      " |      Called when the epoch ends.\n",
      " |  \n",
      " |  on_epoch_start(self)\n",
      " |      Called when the epoch begins.\n",
      " |  \n",
      " |  on_fit_end(self)\n",
      " |      Called when the trainer initialization begins, model has not yet been set.\n",
      " |  \n",
      " |  on_fit_start(self)\n",
      " |      Called when the trainer initialization begins, model has not yet been set.\n",
      " |  \n",
      " |  on_init_end(self)\n",
      " |      Called when the trainer initialization ends, model has not yet been set.\n",
      " |  \n",
      " |  on_init_start(self)\n",
      " |      Called when the trainer initialization begins, model has not yet been set.\n",
      " |  \n",
      " |  on_keyboard_interrupt(self)\n",
      " |      Called when the training is interrupted by KeyboardInterrupt.\n",
      " |  \n",
      " |  on_load_checkpoint(self, checkpoint)\n",
      " |      Called when loading a model checkpoint.\n",
      " |  \n",
      " |  on_pretrain_routine_end(self, model)\n",
      " |      Called when the train ends.\n",
      " |  \n",
      " |  on_pretrain_routine_start(self, model)\n",
      " |      Called when the train begins.\n",
      " |  \n",
      " |  on_sanity_check_end(self)\n",
      " |      Called when the validation sanity check ends.\n",
      " |  \n",
      " |  on_sanity_check_start(self)\n",
      " |      Called when the validation sanity check starts.\n",
      " |  \n",
      " |  on_save_checkpoint(self)\n",
      " |      Called when saving a model checkpoint.\n",
      " |  \n",
      " |  on_test_batch_end(self, outputs, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the test batch ends.\n",
      " |  \n",
      " |  on_test_batch_start(self, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the test batch begins.\n",
      " |  \n",
      " |  on_test_end(self)\n",
      " |      Called when the test ends.\n",
      " |  \n",
      " |  on_test_epoch_end(self)\n",
      " |      Called when the epoch ends.\n",
      " |  \n",
      " |  on_test_epoch_start(self)\n",
      " |      Called when the epoch begins.\n",
      " |  \n",
      " |  on_test_start(self)\n",
      " |      Called when the test begins.\n",
      " |  \n",
      " |  on_train_batch_end(self, outputs, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the training batch ends.\n",
      " |  \n",
      " |  on_train_batch_start(self, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the training batch begins.\n",
      " |  \n",
      " |  on_train_end(self)\n",
      " |      Called when the train ends.\n",
      " |  \n",
      " |  on_train_epoch_end(self, outputs)\n",
      " |      Called when the epoch ends.\n",
      " |  \n",
      " |  on_train_epoch_start(self)\n",
      " |      Called when the epoch begins.\n",
      " |  \n",
      " |  on_train_start(self)\n",
      " |      Called when the train begins.\n",
      " |  \n",
      " |  on_validation_batch_end(self, outputs, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the validation batch ends.\n",
      " |  \n",
      " |  on_validation_batch_start(self, batch, batch_idx, dataloader_idx)\n",
      " |      Called when the validation batch begins.\n",
      " |  \n",
      " |  on_validation_end(self)\n",
      " |      Called when the validation loop ends.\n",
      " |  \n",
      " |  on_validation_epoch_end(self)\n",
      " |      Called when the epoch ends.\n",
      " |  \n",
      " |  on_validation_epoch_start(self)\n",
      " |      Called when the epoch begins.\n",
      " |  \n",
      " |  on_validation_start(self)\n",
      " |      Called when the validation loop begins.\n",
      " |  \n",
      " |  setup(self, model, stage: str)\n",
      " |      Called in the beginning of fit and test\n",
      " |  \n",
      " |  teardown(self, stage: str)\n",
      " |      Called at the end of fit and test\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from pytorch_lightning.trainer.callback_hook.TrainerCallbackHookMixin:\n",
      " |  \n",
      " |  callbacks = []\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.model_hooks.TrainerModelHooksMixin:\n",
      " |  \n",
      " |  has_arg(self, f_name, arg_name)\n",
      " |  \n",
      " |  is_function_implemented(self, f_name, model=None)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.optimizers.TrainerOptimizersMixin:\n",
      " |  \n",
      " |  configure_schedulers(self, schedulers: list, monitor: Union[str, NoneType] = None)\n",
      " |  \n",
      " |  convert_to_lightning_optimizers(self)\n",
      " |  \n",
      " |  init_optimizers(self, model: pytorch_lightning.core.lightning.LightningModule) -> Tuple[List, List, List]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.logging.TrainerLoggingMixin:\n",
      " |  \n",
      " |  metrics_to_scalars(self, metrics)\n",
      " |  \n",
      " |  process_dict_result(self, output, train=False)\n",
      " |      Reduces output according to the training mode.\n",
      " |      \n",
      " |      Separates loss from logging and progress bar metrics\n",
      " |  \n",
      " |  reduce_distributed_output(self, output, num_gpus)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.training_tricks.TrainerTrainingTricksMixin:\n",
      " |  \n",
      " |  detect_nan_tensors(self, loss: torch.Tensor) -> None\n",
      " |  \n",
      " |  print_nan_gradients(self) -> None\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.data_loading.TrainerDataLoadingMixin:\n",
      " |  \n",
      " |  auto_add_sampler(self, dataloader: torch.utils.data.dataloader.DataLoader, shuffle: bool) -> torch.utils.data.dataloader.DataLoader\n",
      " |  \n",
      " |  replace_sampler(self, dataloader, sampler)\n",
      " |  \n",
      " |  request_dataloader(self, dataloader_fx: Callable) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Handles downloading data in the GPU or TPU case.\n",
      " |      \n",
      " |      Args:\n",
      " |          dataloader_fx: The bound dataloader getter\n",
      " |      \n",
      " |      Returns:\n",
      " |          The dataloader\n",
      " |  \n",
      " |  reset_predict_dataloader(self, model) -> None\n",
      " |      Resets the predict dataloader and determines the number of batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The current `LightningModule`\n",
      " |  \n",
      " |  reset_test_dataloader(self, model) -> None\n",
      " |      Resets the test dataloader and determines the number of batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The current `LightningModule`\n",
      " |  \n",
      " |  reset_train_dataloader(self, model: pytorch_lightning.core.lightning.LightningModule) -> None\n",
      " |      Resets the train dataloader and initialises required variables\n",
      " |      (number of batches, when to validate, etc.).\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The current `LightningModule`\n",
      " |  \n",
      " |  reset_val_dataloader(self, model: pytorch_lightning.core.lightning.LightningModule) -> None\n",
      " |      Resets the validation dataloader and determines the number of batches.\n",
      " |      \n",
      " |      Args:\n",
      " |          model: The current `LightningModule`\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pytorch_lightning.trainer.deprecated_api.DeprecatedDistDeviceAttributes:\n",
      " |  \n",
      " |  on_cpu\n",
      " |  \n",
      " |  on_gpu\n",
      " |  \n",
      " |  on_tpu\n",
      " |  \n",
      " |  use_ddp\n",
      " |  \n",
      " |  use_ddp2\n",
      " |  \n",
      " |  use_dp\n",
      " |  \n",
      " |  use_horovod\n",
      " |  \n",
      " |  use_single_gpu\n",
      " |  \n",
      " |  use_tpu\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pytorch_lightning.trainer.deprecated_api.DeprecatedTrainerAttributes:\n",
      " |  \n",
      " |  get_model(self) -> pytorch_lightning.core.lightning.LightningModule\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pytorch_lightning.trainer.deprecated_api.DeprecatedTrainerAttributes:\n",
      " |  \n",
      " |  accelerator_backend\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lr_finder = trainer.lr_find(model)\n",
    "help(pl.Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-19 13:58:07.486523: loading data..\n",
      "2021-02-19 13:58:08.846916: executing target constructor..\n",
      "2021-02-19 13:58:08.858912: executing feature constructor..\n",
      "2021-02-19 13:58:17.235923: shuffling dataset..\n",
      "2021-02-19 13:58:17.274909: defining dataloaders..\n",
      "2021-02-19 13:58:17.275909: Done!\n"
     ]
    }
   ],
   "source": [
    "filename = \"muon_100k_set12_SRT.pt\"\n",
    "dataset, train_loader, test_loader, val_loader = fc.dataset_preparator(name = filename,\n",
    "                                                                       path = path,\n",
    "                                                                       transformer = transformer,\n",
    "                                                                       tc = AzZe_target_constructor,\n",
    "                                                                       fc = fc.custom_feature_constructor,\n",
    "                                                                       shuffle = True,\n",
    "                                                                       TrTV_split = (0,1,1),\n",
    "                                                                       batch_size = args['batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8a3227c99041abbfc7251fb2033d4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'Test Acc': nan}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jv97\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:50: UserWarning: The testing_epoch_end should not return anything as of 9.1. To log, use self.log(...) or self.write(...) directly in the LightningModule\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'Test Acc': nan}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model,test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
